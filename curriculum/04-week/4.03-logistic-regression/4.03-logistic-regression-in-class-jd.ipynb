{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title: Introduction to Logistic Regression\n",
    "#### Creators: Arun Ahuja (NYC) + Matt Brems (DC)\n",
    "---\n",
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Introduction to Logistic Regression\n",
    "Week 4 | Lesson 4.03\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Define logistic regression\n",
    "- Explain the math behind logistic regression\n",
    "- Build logistic regression models using statsmodels\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Why is this necessary?\n",
    "\n",
    "Suppose we have a dataset with two variables in it: one variable we seek to predict (Y) and one independent variable (X) we want to use in order to predict Y. What we would have done previously is to pick a metric (usually mean squared error) and then fit a line of best fit.\n",
    "\n",
    "**However, what if Y is binary?**\n",
    "\n",
    "After turning our binary variable into a dummy variable appropriate for use in Python, our plot might look something like this.\n",
    "\n",
    "![](binary_y_plot.jpg)\n",
    "\n",
    "If we were to fit a traditional line through it, we'd likely get a line that fits as follows:\n",
    "\n",
    "![](binary_y_with_linear_regression.jpg)\n",
    "\n",
    "Are there any problems associated with this?\n",
    "\n",
    "Wouldn't it be swell to get a line that looked like this?\n",
    "\n",
    "![](binary_y_with_logistic_regression.jpg)\n",
    "\n",
    "Logistic regression is a linear approach to solving a binary classification problem. That is, we can use a linear model, similar to linear regression, in order to estimate the probability that an observation falls into a particular category and, based on this probability, provide a \"best guess\" for in which category the observation likeliest falls.\n",
    "\n",
    "Before going further into logistic regression, let's step back and discuss *generalized linear models.*\n",
    "\n",
    "### Generalized Linear Models (GLMs)\n",
    "\n",
    "**Generalized linear models** is a class of models that take the linear model (think linear regression) and *generalize* it beyond the assumptions of the linear regression model we discussed the other day. \n",
    "\n",
    "**Check:** What are the assumptions of a simple linear regression model? How about a multiple linear regression model?\n",
    "\n",
    "There are three components to every GLM:\n",
    "- Linear Component (We still assume that a one-unit change in $X$ changes the outcome by a specific amount.)\n",
    "- Link Component (How do we transform our linear component to match the desired outcome?)\n",
    "- Random Component (How are the errors distributed?)\n",
    "\n",
    "[Wikipedia Entry on GLM](https://en.wikipedia.org/wiki/Generalized_linear_model)\n",
    "\n",
    "Two notes for GLMs:\n",
    "1. Generalized linear models $\\neq$ [general linear models](https://en.wikipedia.org/wiki/General_linear_model). (GLM will always refer to generalIZED linear models!)\n",
    "2. Our metrics for linear regression don't easily translate to GLMs. We can statistically compare two models using [**deviance**](https://en.wikipedia.org/wiki/Deviance_(statistics)), which is a generalization of sum of squares of error.\n",
    "\n",
    "[Statsmodels GLM Documentation](http://statsmodels.sourceforge.net/devel/glm.html)\n",
    "\n",
    "\n",
    "## What is Logistic Regression? A Short Definition\n",
    "\n",
    "Logistic regression measures the relationship between the dependent variables and one or more independent variables by estimating probabilities.\n",
    "\n",
    "The goal of logistic regression is to predict the probability that $Y = 1$ given certain values of $X$. That is, if $X$ and $Y$ have a positive linear relationship, the probability that $Y = 1$ will increase as values of $X$ increase. Our model's goal is predicting probabilities - rather than target scores - with our independent variables. Once we get these predicted probabilities, we can classify observations as falling into one category or the other based on the predicted probability.\n",
    "\n",
    "For example, we might try to predict whether or not small businesses will succeed or fail based on the number of years of experience the owner has in the field prior to starting the business. We presume that those people who have been selling widgets for many years who open their own widget business will be more likely to succeed.\n",
    "\n",
    "That means that as $X$ (the number of years of experience) increases, the probability that $Y = 1$ (success in the new widget business) will tend to increase.\n",
    "\n",
    "### Intuition Behind Logistic Regression\n",
    "\n",
    "The intuition behind logistic regression is to transform the output of a linear regression ($Y$) which generally can be any value, to a range appropriate for probability - that is, anywhere between 0 and 1. \n",
    "\n",
    "The specific transformation used to \"bend\" our linear regression line is called the logit and gives rise to the name \"logistic regression.\" (Note: this is our *link* function from above!) It can predict values between 0 and 1, but does not include either.\n",
    "\n",
    "![](Logit Walkthrough.jpg)\n",
    "\n",
    "(For more on why probability practically cannot be absolute, see http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/)\n",
    "\n",
    "## *Some* of the Math behind Logistic Regression\n",
    "\n",
    "Logistic regression is only appropriate when the dependent variable is binary; for example, when something can be coded as $0$ or $1$.\n",
    "\n",
    "**Check:** What are some examples of situations where this would be appropriate?\n",
    "\n",
    "##### Binary expected value and odds ratios\n",
    "\n",
    "When $Y$ is binary, the expected value of $Y$ is equivalent to $P(Y=1)$, as we discussed above.\n",
    "\n",
    "Probability is sometimes expressed as odds which is simply the probability of one event or category divided by 1 - that probability. In horse racing:\n",
    "\n",
    "$$ Odds(winning) = \\frac{P(winning)}{P(losing)} = \\frac{P(winning)}{1 - P(winning)} $$\n",
    "\n",
    "Suppose the probability of having lung cancer given that you smoke at least one pack of cigarettes a day is 25% and the probability of having lung cancer given that you do not smoke at least one pack of cigarettes a day is 5%.\n",
    "\n",
    "Then:\n",
    "- The odds of having cancer given that you smoke is 25% / 75% or 1/3.\n",
    "- The odds of having cancer given that you do not smoke is 5% / 95% or 1/19.\n",
    "- The odds ratio of cancer associated with smoking is 1/3 / 1/19 = 19/3 = 6.33.\n",
    "- We interpret this as \"You are 6.33 (repeating, of course) times as likely to have lung cancer given that you smoke at least one pack of cigarettes a day.\"\n",
    "\n",
    "The coefficients $\\beta_1$, $\\beta_2$, $\\ldots$ are equal to the **log of the odds ratios**.\n",
    "\n",
    "The values $e^{\\beta_1}$, $e^{\\beta_2}$, $\\ldots$ are therefore equal to the **odds ratios** themselves.\n",
    "\n",
    "Specifically, $e^{\\beta_1}$ will give you the odds ratio associated with a one-unit change in $X_1$. (Sometimes people use log-odds ratios but odds ratios are generally more easily interpreted.) **To be clear, we interpret this as \"As $X_1$ increases by one unit, an individual is $e^{\\beta_1}$ times as likely to [fall into class $Y = 1$].\"**\n",
    "\n",
    "*Note: When we estimate coefficients, we use hats to indicate this. If we have attempted to estimate the true value of $\\beta_1$, we might use $\\hat{\\beta_1}$ to denote the estimated value.*\n",
    "\n",
    "Note that your coefficients can take on any value - positive, negative, zero, whatever - but our predicted $Y$ value is now bound between 0 and 1. This is very convenient!\n",
    "\n",
    "## Recap & Review\n",
    "\n",
    "Check: What is logistic regression?\n",
    "\n",
    "Check: What is important about the dependent variable for logistic regression?\n",
    "\n",
    "#### Goals of Logistic Regression\n",
    "\n",
    "The goal of logistic regression is to find the best fitting model to describe the relationship between the binary characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest.\n",
    "\n",
    "Logistic regression is quite simple. Specify the column containing the variable you're trying to predict followed by the columns that the model should use to make the prediction.\n",
    "\n",
    "We will use a dataset of chemical properties of wines to predict those that are high quality or not.\n",
    "\n",
    "## Guided Practice: Topic\n",
    "\n",
    "Use the following Python code to demonstrate logistic regression.  We will utilize the formula syntax of `statsmodels` to automatically create dummy columns.\n",
    "\n",
    "```python\n",
    "import statsmodels.formula.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/demo-datasets/wine.csv\")\n",
    "\n",
    "model = sm.logit(\n",
    "    \"high_quality ~ residual_sugar + pH + alcohol\",\n",
    "    data = df\n",
    ").fit()\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Check: Understanding of dummy variables and summary table.**\n",
    "\n",
    "Let's check out the [Starter Code](./starter-code.ipynb).\n",
    "\n",
    "## Conclusion\n",
    "- Logistic regression is a classification algorithm with similar properties to linear regression.\n",
    "- It is very fast and efficient and is by far the most common classification algorithm.\n",
    "- The coefficients in a logistic regression model represent the change in log-odds due to the input variables.\n",
    "\n",
    "### ADDITIONAL RESOURCES\n",
    "- [Logistic Regression Video Walkthrough](https://www.youtube.com/watch?v=zAULhNrnuL4&noredirect=1)\n",
    "- [Logistic Regression Walkthrough](http://www.mc.vanderbilt.edu/gcrc/workshop_files/2004-11-12.pdf)\n",
    "- [Logistic Regression w/ Statsmodel - Well Switching in Bangledesh](http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/ARM/ch5/arsenic_wells_switching.ipynb)\n",
    "- [Odds Ratio Explanation](http://www.wright.edu/~thaddeus.tarpey/ES714glm.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

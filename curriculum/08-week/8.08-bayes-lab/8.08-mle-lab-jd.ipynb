{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun time with Bayesian statistics and Politics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://a.abcnews.go.com/images/Politics/GTY_donald_trump_hillary_clinton_sk_150619_16x9_992.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://a.abcnews.go.com/images/Politics/GTY_donald_trump_hillary_clinton_sk_150619_16x9_992.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presidential voting season was recently upon us, and what better way to celebrate it (lol), and our newfound understanding of Bayesian statistics by fusing the two together? In the 2012 last presidential electoral cycle, Nate Silver won acclaim (and some scorn from academia for the acclaim he garnered with work that lacked much novelty) for utilizing a Bayesian approach to his poll prediction, in particular he utilized something called a Monte Carlo Markov Chain (MCMC). We're going to start our nascent political data science careers in this lab with something slightly less ambitious, but no less relevant to the material we've gone through so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some model background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Priors and Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conjugate Priors\n",
    "| Prior  | Likelihood  | \n",
    "|---|---|\n",
    " | Beta  | Binomial  |\n",
    " | Dirichlet  | Multinomial   | \n",
    " | Gamma  | Gaussian |\n",
    " | Gaussian  | Gaussian | \n",
    " \n",
    " The above table provides the likelihood/prior conjugate prior combinations that ensure computational tractability for calculation of posterior distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen some of these previously, but let's just summarize these distributions now: \n",
    "\n",
    "| Name  | Function (The Distribution/PMF) - Whatever| Use\n",
    "|---|---|\n",
    "| Beta  | $\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$ | More of a family of distributions, usually representing probabilties of proportions, thus it's great to use as our prior belief (very important in Bayesian analysis)|\n",
    "| Dirichlet  |  $\\begin{equation*}\\frac{1}{B(\\alpha, ,\\beta)}\\prod_{i = 1}^{n}x_i^{\\alpha-1}\\end{equation*}$  | Similar to Beta above, just extended over multinomials, often used in NLP and other text-mining processes  |\n",
    "| Gamma  |$ x^{a-i}e^{-x}\\frac{1}{\\Gamma(a)}$ | This distribution represents waiting time between Poisson distributed events (remember c_i in the previous lecture finger exercise | \n",
    "| Gaussian  | $\\frac{1}{\\sigma \\sqrt(2\\pi)}e^{\\frac{-(x-\\mu)^{2}}{2\\sigma^2}}$ | This is just the normal distribution | \n",
    "| Binomial  | $\\binom{n}{k}\\cdot p^kq^{n-k} $   | Gives you the probability of getting k \"success\" in n iterations/trials |\n",
    "| Multinomial  | $\\frac{N!}{\\prod_{i=1}^n x_i!} \\prod_{i=1}^n \\theta_i^{x_i} $| Similar to binomial distribution, but generalized to include multiple \"buckets\" (X, Y, Z,...) instead of just two (1 or 0), represents how many times each bucket X, Y, Z,... etc was observed over n trials | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Beta functions as normalizing term in case you don't want to make your life easy and use conjugate priors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally going to use the much anticipated Beta Function today to do our first real computational Bayesian exercise, $\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$. Wait, what's B? We can define B as the following: \n",
    "$$B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$$\n",
    "\n",
    "As it is used in Bayesian Analysis, the beta distribution (not \"B\" - or the beta function) is often used to represent the prior beliefs of the modeler. How so? Well let's say you thought the chance of a political candidate winning an election was .60 (whatever that means), but that given new information, that you've cleverly modeled using Bayes, it's now .80 (say you've tied your candidates political fortunes to the performance of the economy), your prior beliefs can be modeled (after some tuning of $\\alpha$, $\\beta$, the input variable) by the beta distribution.\n",
    "\n",
    "**The beta function** in contrast, is often used as a scaling term that ensures your model is bounded by (0, 1).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build the Beta function into Python using the math library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import gamma as Gamma\n",
    "from scipy.stats import beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 (An advanced preview) Introducing Dirchilet Distribution for Multi-level Categorical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the longer term, our main aim is to model the quotient, in particular, the percentage of voters for a particular candidate. The 'commonest' way to do this (to borrow a phrase from our British friends), is to utilize the Dirchilet distribution in our probability model: $\\begin{equation*}\\frac{1}{B(\\alpha)}\\prod_{i = 1}^{n}x_i^{\\alpha-1}\\end{equation*}$ (stay tuned for this later)  \n",
    "\n",
    "As you read in the summary in the table above, it's often used to model situations with more than \"2\" possiblities/buckets/outcomes/whatever. This is in fact a very powerful distribution. \n",
    "\n",
    "For our purposes, we would use this distribution to model situations where there are more than 2 candidates (Clinton, Bush, Perot in 1992, or maybe Clinton, Trump, Sanders in 2016. But for this lab,  let's assume there's just two alternatives. That way, we can utilize the simple binomial model and as our table states, we just need to use beta for that case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This case study will be based off of the following data: http://www.stat.columbia.edu/~gelman/arm/examples/election88/, from the **great one**, Dr. Gelman, one of the foremost researchers in Bayesian analysis. \n",
    "\n",
    "This election was ultimately a major victory for the Grand Old Party (GOP/Republicans), and propelled George H.W. Bush (the father of George W. Bush) into the presidency. \n",
    "\n",
    "Import the data, and delete the Unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading in the csv file\n",
    "e88 = pd.read_csv('election88.csv')\n",
    "polls = pd.read_csv('polls.csv')\n",
    "\n",
    "# Calling a conversion method of your data to csv \n",
    "\n",
    "\n",
    "# The first column \"Unnamed\" is clearly just a row-identifer machine generated. We can ignore that column or drop it all together\n",
    "e88.drop(e88.columns.values[0],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Further, we can remove org, as it is just one value -- Special note, in general, we probably don't want to do this, because \n",
    "# there may be a chance we want to merge/join this file with another and that column could end up being a unique identifier for \n",
    "# a subset of data in our large dataset\n",
    "polls.drop('org',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data read in, we would like to partition the data into states so we can do state-specific inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Summarize the age variable with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4788\n",
       "3    3429\n",
       "1    3149\n",
       "4    2178\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of yers of the data\n",
    "polls.age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Summarize the state with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 distinct entities/states\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of entities/states of the data\n",
    "print str(len(set(polls.state))) + ' distinct entities/states'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cretae a data frame with only state and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We need to partition the data into individual data frames to construct our Bayesian inference on \n",
    "state_age = polls[['state','age']]\n",
    "state_age_grouped = polls[['state','age']].groupby(['state']).agg({'age':np.mean}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great, now let's do some basic exploratory analysis on the polling data. Let's construct the Bush support for state-age cohorts, and save that data for use in the dictionary, container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "container = {}; bush_quotient = 0\n",
    "\n",
    "bush_age = polls.groupby(['age','bush'])['weight'].sum().groupby(level = 0).transform(lambda x: x/x.sum()).reset_index()\n",
    "bush_age = bush_age[['age','weight']][bush_age.bush==1]\n",
    "\n",
    "\n",
    "# Likewise, let's also produce a dictionary with the general poll percentages for state (no age)\n",
    "\n",
    "state_sum = polls.groupby(['state','bush'])['weight'].sum().groupby(level = 0).transform(lambda x: x/x.sum()).reset_index()\n",
    "state_sum = state_sum[['state','weight']][state_sum.bush==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's just check out what # 1 shows (all ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stnum</th>\n",
       "      <th>st</th>\n",
       "      <th>electionresult</th>\n",
       "      <th>samplesize</th>\n",
       "      <th>raking</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0.59</td>\n",
       "      <td>203</td>\n",
       "      <td>0.673067</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AK</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AZ</td>\n",
       "      <td>0.60</td>\n",
       "      <td>194</td>\n",
       "      <td>0.568980</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AR</td>\n",
       "      <td>0.56</td>\n",
       "      <td>121</td>\n",
       "      <td>0.563672</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CA</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.531725</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>CO</td>\n",
       "      <td>0.53</td>\n",
       "      <td>181</td>\n",
       "      <td>0.599325</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>CT</td>\n",
       "      <td>0.52</td>\n",
       "      <td>171</td>\n",
       "      <td>0.519580</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>DE</td>\n",
       "      <td>0.56</td>\n",
       "      <td>39</td>\n",
       "      <td>0.444178</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>DC</td>\n",
       "      <td>0.14</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>FL</td>\n",
       "      <td>0.61</td>\n",
       "      <td>750</td>\n",
       "      <td>0.623274</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.60</td>\n",
       "      <td>316</td>\n",
       "      <td>0.537512</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>HI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>ID</td>\n",
       "      <td>0.62</td>\n",
       "      <td>42</td>\n",
       "      <td>0.438164</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>IL</td>\n",
       "      <td>0.51</td>\n",
       "      <td>567</td>\n",
       "      <td>0.496166</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>IN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>291</td>\n",
       "      <td>0.682633</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>IA</td>\n",
       "      <td>0.45</td>\n",
       "      <td>143</td>\n",
       "      <td>0.387524</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>KS</td>\n",
       "      <td>0.56</td>\n",
       "      <td>141</td>\n",
       "      <td>0.658353</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>KY</td>\n",
       "      <td>0.56</td>\n",
       "      <td>210</td>\n",
       "      <td>0.650197</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>LA</td>\n",
       "      <td>0.54</td>\n",
       "      <td>196</td>\n",
       "      <td>0.574370</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>ME</td>\n",
       "      <td>0.55</td>\n",
       "      <td>51</td>\n",
       "      <td>0.553497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>MD</td>\n",
       "      <td>0.51</td>\n",
       "      <td>284</td>\n",
       "      <td>0.451037</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>MA</td>\n",
       "      <td>0.45</td>\n",
       "      <td>373</td>\n",
       "      <td>0.459859</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>MI</td>\n",
       "      <td>0.54</td>\n",
       "      <td>530</td>\n",
       "      <td>0.523901</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>MN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>289</td>\n",
       "      <td>0.468303</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>MS</td>\n",
       "      <td>0.60</td>\n",
       "      <td>220</td>\n",
       "      <td>0.608176</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>MO</td>\n",
       "      <td>0.52</td>\n",
       "      <td>309</td>\n",
       "      <td>0.448425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>MT</td>\n",
       "      <td>0.52</td>\n",
       "      <td>40</td>\n",
       "      <td>0.371521</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>NE</td>\n",
       "      <td>0.60</td>\n",
       "      <td>125</td>\n",
       "      <td>0.595187</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>NV</td>\n",
       "      <td>0.59</td>\n",
       "      <td>32</td>\n",
       "      <td>0.512611</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>NH</td>\n",
       "      <td>0.62</td>\n",
       "      <td>27</td>\n",
       "      <td>0.688476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>NJ</td>\n",
       "      <td>0.56</td>\n",
       "      <td>428</td>\n",
       "      <td>0.571948</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>NM</td>\n",
       "      <td>0.52</td>\n",
       "      <td>109</td>\n",
       "      <td>0.532814</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>NY</td>\n",
       "      <td>0.48</td>\n",
       "      <td>894</td>\n",
       "      <td>0.456643</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.58</td>\n",
       "      <td>346</td>\n",
       "      <td>0.617115</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>ND</td>\n",
       "      <td>0.56</td>\n",
       "      <td>60</td>\n",
       "      <td>0.640970</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>OH</td>\n",
       "      <td>0.55</td>\n",
       "      <td>605</td>\n",
       "      <td>0.626706</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>OK</td>\n",
       "      <td>0.58</td>\n",
       "      <td>130</td>\n",
       "      <td>0.507765</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>OR</td>\n",
       "      <td>0.47</td>\n",
       "      <td>149</td>\n",
       "      <td>0.499180</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.51</td>\n",
       "      <td>616</td>\n",
       "      <td>0.497529</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.44</td>\n",
       "      <td>91</td>\n",
       "      <td>0.424055</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>SC</td>\n",
       "      <td>0.62</td>\n",
       "      <td>223</td>\n",
       "      <td>0.616524</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.53</td>\n",
       "      <td>60</td>\n",
       "      <td>0.505656</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>TN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>329</td>\n",
       "      <td>0.668285</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>TX</td>\n",
       "      <td>0.56</td>\n",
       "      <td>788</td>\n",
       "      <td>0.531733</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>UT</td>\n",
       "      <td>0.66</td>\n",
       "      <td>79</td>\n",
       "      <td>0.812587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>VT</td>\n",
       "      <td>0.51</td>\n",
       "      <td>12</td>\n",
       "      <td>0.802644</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>VA</td>\n",
       "      <td>0.60</td>\n",
       "      <td>354</td>\n",
       "      <td>0.624666</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>WA</td>\n",
       "      <td>0.49</td>\n",
       "      <td>393</td>\n",
       "      <td>0.473773</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>WV</td>\n",
       "      <td>0.48</td>\n",
       "      <td>117</td>\n",
       "      <td>0.534503</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>WI</td>\n",
       "      <td>0.48</td>\n",
       "      <td>389</td>\n",
       "      <td>0.527082</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>WY</td>\n",
       "      <td>0.61</td>\n",
       "      <td>15</td>\n",
       "      <td>0.568052</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stnum  st  electionresult  samplesize    raking  _merge\n",
       "0       1  AL            0.59         203  0.673067       3\n",
       "1       2  AK            0.60           0       NaN       1\n",
       "2       3  AZ            0.60         194  0.568980       3\n",
       "3       4  AR            0.56         121  0.563672       3\n",
       "4       5  CA            0.51        1493  0.531725       3\n",
       "5       6  CO            0.53         181  0.599325       3\n",
       "6       7  CT            0.52         171  0.519580       3\n",
       "7       8  DE            0.56          39  0.444178       3\n",
       "8       9  DC            0.14          19       NaN       1\n",
       "9      10  FL            0.61         750  0.623274       3\n",
       "10     11  GA            0.60         316  0.537512       3\n",
       "11     12  HI            0.45           0       NaN       1\n",
       "12     13  ID            0.62          42  0.438164       3\n",
       "13     14  IL            0.51         567  0.496166       3\n",
       "14     15  IN            0.60         291  0.682633       3\n",
       "15     16  IA            0.45         143  0.387524       3\n",
       "16     17  KS            0.56         141  0.658353       3\n",
       "17     18  KY            0.56         210  0.650197       3\n",
       "18     19  LA            0.54         196  0.574370       3\n",
       "19     20  ME            0.55          51  0.553497       3\n",
       "20     21  MD            0.51         284  0.451037       3\n",
       "21     22  MA            0.45         373  0.459859       3\n",
       "22     23  MI            0.54         530  0.523901       3\n",
       "23     24  MN            0.46         289  0.468303       3\n",
       "24     25  MS            0.60         220  0.608176       3\n",
       "25     26  MO            0.52         309  0.448425       3\n",
       "26     27  MT            0.52          40  0.371521       3\n",
       "27     28  NE            0.60         125  0.595187       3\n",
       "28     29  NV            0.59          32  0.512611       3\n",
       "29     30  NH            0.62          27  0.688476       3\n",
       "30     31  NJ            0.56         428  0.571948       3\n",
       "31     32  NM            0.52         109  0.532814       3\n",
       "32     33  NY            0.48         894  0.456643       3\n",
       "33     34  NC            0.58         346  0.617115       3\n",
       "34     35  ND            0.56          60  0.640970       3\n",
       "35     36  OH            0.55         605  0.626706       3\n",
       "36     37  OK            0.58         130  0.507765       3\n",
       "37     38  OR            0.47         149  0.499180       3\n",
       "38     39  PA            0.51         616  0.497529       3\n",
       "39     40  RI            0.44          91  0.424055       3\n",
       "40     41  SC            0.62         223  0.616524       3\n",
       "41     42  SD            0.53          60  0.505656       3\n",
       "42     43  TN            0.58         329  0.668285       3\n",
       "43     44  TX            0.56         788  0.531733       3\n",
       "44     45  UT            0.66          79  0.812587       3\n",
       "45     46  VT            0.51          12  0.802644       3\n",
       "46     47  VA            0.60         354  0.624666       3\n",
       "47     48  WA            0.49         393  0.473773       3\n",
       "48     49  WV            0.48         117  0.534503       3\n",
       "49     50  WI            0.48         389  0.527082       3\n",
       "50     51  WY            0.61          15  0.568052       3"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also have the actual Election 88 sample percentages to compare with the polling. Let's load the data just to have a look\n",
    "import pandas as pd\n",
    "\n",
    "e88\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the electoral data shows that the actual results were .59, whereas the polling showed .71. Does that mean Bush somehow lost almost 17% of his support from the time of the poll to the election? We can put some clarify on this question by doing a simple Bayesian computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining The Prior and Likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the table way back at the top that using Beta-Binomial Prior-Likelihood combination allows us to remove the denominator/scaling term, and makes the right-hand side of the Bayesian formula analytically tractable (we can use a Riemann or 'normal' Integral or a simple summation) to compute the probability. Wonderful, polling data is naturally contextualized using Binomial distribution! \n",
    "\n",
    "So let's test whether or not $\\theta \\le .65$, that would basically be the following closed form equation : \n",
    "$$P(\\theta \\le .65) = \\frac{\\Gamma( 404 )}{\\Gamma(141)\\Gamma (263)} \\int_{0}^{.65} \\theta^{144}(1-\\theta)^{59}\\theta^{119}(1-\\theta)^{82}$$\n",
    "\n",
    "Note: A closed form equation is just a fancy way to mean you can write write something as A = **blah**, where **blah** is a formula that can be computed and is not infinite. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "from scipy.stats import beta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Uh oh, doesn't work.... what happened? Well, the gamma funciton is not computaitonally tractable. We'll need to figure out, A different way to compute Beta..... what can that be? \n",
    "\n",
    "What does tractability mean in data science/computer science? First let's look deeper at my_beta above. Note we use the method gamma to construct beta. This method refers to the gamma **function** and not the distribution. What is gamma? \n",
    "\n",
    "Please read the following:\n",
    "http://mathworld.wolfram.com/GammaFunction.html\n",
    "\n",
    "Note $\\Gamma(a)=(a-1)!$, and if you recall (a-1)! = (a-1)(a-2)...(2)(1). For fun, try to calculate $\\Gamma(90)$, $\\Gamma(100)$, anything funny happen? Overflow you say? Yep. Gamma explodes very quickly. This is bad for computers (or more precisely Von-Neumann computers -which we all currently use). \n",
    "\n",
    "Therefore, we need to find some trick around this explosion. What about logs? Take My_beta and take the log (something you should have had some practice with so far). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Towards The Future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that the $P(\\theta \\le .65)$ is very close to 0, and therefore we believe the poll did not provide \"good\" information with respect to the actual electoral results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

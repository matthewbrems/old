{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: MRJob lab\n",
    "type: lab\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) MRJob Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the past lab we've used a Virtual Machine to run Map Reduce jobs on native hadoop. As you may have understood, it's quite cumbersome and complicated.\n",
    "\n",
    "Luckily we don't have to do that, because our friends at Yelp developed a great open source python library that wraps around hadoop streaming called [MRJob](https://github.com/Yelp/mrjob).\n",
    "\n",
    "This is already installed in your VM, but you can also install it locally if you prefer, using:\n",
    "\n",
    "`pip install mrjob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): mrjob in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): google-api-python-client>=1.5.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): PyYAML>=3.08 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): boto>=2.35.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): filechunkio in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): uritemplate<1,>=0.6 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six<2,>=1.6.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): httplib2<1,>=0.8 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): oauth2client in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): simplejson>=2.5.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from uritemplate<1,>=0.6->google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyasn1>=0.1.7 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyasn1-modules>=0.0.5 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): rsa>=3.1.4 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob\n",
    "\n",
    "While Hadoop streaming is a simple way to do simple map-reduce tasks, it's complicated to use and not really friendly when things fail and we have to debug our code. Also, if we wanted to do a join from two different sets of data, it would be complicate to handle both with a single mapper, while it'd be much easier to have two separate mappers and one reducer.\n",
    "\n",
    "_MRJob_ is a library written and maintained by YELP that allows us to write map reduce jobs in python.\n",
    "\n",
    "Here's the word count map reduce, rewritten using MRJob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audible: Enter Fun\n",
    "\n",
    "You are working for a client that insists you run all of these jobs on their Virtual Machine for security reasons and not your local machine. This isn't so bad because they have the necessary packages and information there.\n",
    "\n",
    "Typically you execute you code in an IDE, however within the client VM you have access to nothing outside of things that can be executed in the command line.  \n",
    "\n",
    "A coworker of yours has provided **most** of the code for a very similar project they worked on so you don't have to figure out how to solve answer the clients questions you just need to figure out a way to execute the code.\n",
    "\n",
    "Possible solutions to this would be to...\n",
    "- Find a way/means to execute the necessary code in the terminal\n",
    "- Find a way to connect a IDE, like Jupyter or Spyder on your local machine to the VM \n",
    "\n",
    "\n",
    "**Bonus**:\n",
    "- Your coworker is a lazy millennial who doesn't comment their code.  Comment in what their code is doing to prove to your boss you know it better than them. \n",
    "\n",
    "\n",
    "_tl:dr- You have the code but no means of executing it.  figure out what it does and execute._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordfrequency.py\n"
     ]
    }
   ],
   "source": [
    "# This block of text you will have no issue running on your local machine.\n",
    "\n",
    "%%file wordfrequency.py\n",
    "\n",
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it's almost trivial: the mapper and the reducer become methods of our `MRWordFreqCount` class that inherits from the MRJob class.\n",
    "\n",
    "The script can be run in local mode with the following command:\n",
    "\n",
    "```bash\n",
    "python starter_code/wordfrequency.py data/project_gutenberg/1184-0.txt\n",
    "```\n",
    "\n",
    "Notice that it executes immediately on our VM. This is because if run in local mode MRJob will not use Hadoop to do the map-reduce. This will be very fast on small data, but become increasingly slow if data size increases.\n",
    "\n",
    "We can switch to hadoop mode by simply using the flag `-r hadoop` like this:\n",
    "\n",
    "```bash\n",
    "python starter_code/wordfrequency.py data/project_gutenberg/1184-0.txt -r hadoop\n",
    "```\n",
    "\n",
    "As you can see this wraps around hadoop streaming and it automates all the steps we did manually in the previous lecture including:\n",
    "\n",
    "- copying data to hdfs\n",
    "- running the data through hadoop streaming\n",
    "- copying back the output\n",
    "- removing temporary folders from hdfs\n",
    "\n",
    "Nice!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Use the code above to run the Word count on the whole project_gutenberg folder using MRJob\n",
    "1. Compare the execution time for the local mode and the hadoop mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: \n",
    ">\n",
    "    python scripts/mrjob_wc.py data/project_gutenberg\n",
    "    python scripts/mrjob_wc.py data/project_gutenberg -r hadoop\n",
    "    \n",
    "    \n",
    "_These should come in handy when trying to eccess the data for this lab._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: add a combiner\n",
    "\n",
    "\n",
    "A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.\n",
    "\n",
    "The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.\n",
    "\n",
    "In MRJob these can be added simply by defining a method called `combiner`. Go ahead and modify the `MR` class adding a combiner. Then run it on the `project_gutenberg` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "# There is something about this combiner aspect of our MRjobs class that starts to screw things up locally.\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: multi step jobs\n",
    "\n",
    "So far we've always worked with one step Map-reduce jobs. These are very simple. What if we wanted to perform a calculation that involves multiple steps? For example, what if we wanted to count the words in our documents and then find the most common word? This would involve the following steps:\n",
    "\n",
    "- map our text to a mapper that output pairs of (word, 1)\n",
    "- combine the pairs using the word as key (optional)\n",
    "- reduce the pairs using the word as key\n",
    "- find the word with the maximum count\n",
    "\n",
    "The last step can be achieved by chaining a new map reduce where the map function is the identity and the reduce function is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer_find_max_word(self, _, word_count_pairs):\n",
    "    yield max(word_count_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are aggregating over a blank key in order to get all possible word count pairs and then get the one with the maximum count.\n",
    "\n",
    "Go ahead and insert that into the MR class. In order to do that you'll need to use the `steps` function which is documented [here](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Setup and teardown of tasks\n",
    "\n",
    "MRJob allows you to write methods to run at the beginning and end of each mapper and reducer functions: the *_init() and *_final() methods:\n",
    "\n",
    "- mapper_init()\n",
    "- combiner_init()\n",
    "- reducer_init()\n",
    "- mapper_final()\n",
    "- combiner_final()\n",
    "- reducer_final()\n",
    "\n",
    "These functions are run only once at the beginning or at the end of each mapper and reducer and are useful for example when we want to have local variables available to the mapper.\n",
    "\n",
    "For example we could use `mapper_init` to load some kind of support file, like a sqlite3 database, or perhaps create a temporary fileor variable.\n",
    "\n",
    "Go ahead and rewrite the Word count using a `mapper_init` and `mapper_final` that do the following:\n",
    "\n",
    "- mapper_init should initialize a dictionary for the words\n",
    "- mapper should add the words to the dictionary as keys and increase the count each time the same word is ecountered\n",
    "- mapper_final should yield the (word, count) pairs contained in the dictionary\n",
    "- the reducer is going to be the same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def init_get_words(self):\n",
    "        self.words = {}\n",
    "\n",
    "    def get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            word = word.lower()\n",
    "            self.words.setdefault(word, 0)\n",
    "            self.words[word] = self.words[word] + 1\n",
    "\n",
    "    def final_get_words(self):\n",
    "        for word, val in self.words.iteritems():\n",
    "            yield word, val\n",
    "\n",
    "    def sum_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.init_get_words,\n",
    "                       mapper=self.get_words,\n",
    "                       mapper_final=self.final_get_words,\n",
    "                       combiner=self.sum_words,\n",
    "                       reducer=self.sum_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Counters\n",
    "\n",
    "When we run a longer MR task, we may want to check the status of the calculation. This is achieved in MRJob using counters.\n",
    "\n",
    "Hadoop lets you track counters that are aggregated over a step. A counter has a group, a name, and an integer value. Hadoop itself tracks a few counters automatically. mrjob prints your job’s counters to the command line when your job finishes, and they are available to the runner object if you invoke it programmatically.\n",
    "\n",
    "To increment a counter from anywhere in your job, use the `increment_counter()` method.\n",
    "\n",
    "Go ahead and add a custom counter to the mapper function of the word count so that we know how many words it has processed. In order to see the counts, you may want to redirect the output of the job to a file or to `/dev/null` if you don't care about it.\n",
    "\n",
    "    python mrjob_counters.py data/project_gutenberg > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter('my_group', 'my_special_mapper_counter', 1)\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('my_group', 'my_special_reducer_counter', 1)\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Putting it all together\n",
    "\n",
    "You've seen how powerful MRJob can be. Let's put it all together and write a class that returns the top 15 most frequent words in our text. We can implement this in various ways, you're free to choose the one you think is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Use NLTK to recognize a book\n",
    "\n",
    "Let's see if we can recognize a book from the most common words.\n",
    "\n",
    "- Run a local MRJob word count on a single file in the project_gutemberg\n",
    "- save the result to a pandas dataframe\n",
    "- use nltk to filter out the common english words\n",
    "- print out the most common 40 words\n",
    "- what book is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Additional Resources\n",
    "\n",
    "- [MRJob Documentation](https://pythonhosted.org/mrjob/)\n",
    "- [MRJob Examples](https://github.com/Yelp/mrjob/tree/master/mrjob/examples)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab with pipelines\n",
    "\n",
    "In this lab you will try out pipelines with what you've learned so far and practice logistic regression on news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv('../assets/dataset/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cleaning\n",
    "There's quite a lot of stuff in the dataset. For a more detailed description of what everything is, you can find the data dictionary here: https://www.kaggle.com/c/stumbleupon/data\n",
    "\n",
    "For the purposes of this exercise, we're interested in 'boilerplate' (our predictor) and 'label' (our target). In this case, the target is binary, indicating whether something is evergreen - read over and over again - or not. \n",
    "\n",
    "You may want to clean up the 'boilerplate' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the title and body from the boilerplate JSON text\n",
    "df['title'] = df.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "df['body'] = df.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.33) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Pipeline\n",
    "\n",
    "Try out making pipelines with different transformations (look at the scikit-learn documentation for some that you think would be good) with a LogisticRegression instance. \n",
    "\n",
    "Notice that a `sklearn.pipeline` can have an arbitrary number of transformation steps, but only one, optional, estimator step as the last one in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__analyzer': ('word', 'char'),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2, 2), (3, 3), (4, 4)),  # individually checking uni- through tetragrams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, scoring='precision_macro', n_jobs=-1, verbose=1) # precision-- minimize false-positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model\n",
    "Use `X_train` and `y_train` to fit the model.\n",
    "Use `X_test` to generate predicted values for the target variable and save those in a new variable called `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   13.0s finished\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__analyzer': 'char', 'vect__ngram_range': (4, 4), 'vect__max_df': 0.5}\n",
      "0.767165459613\n"
     ]
    }
   ],
   "source": [
    "print model.best_params_\n",
    "print model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the model accuracy\n",
    "\n",
    "1. Use the `confusion_matrix` and `classification_report` functions to assess the quality of the model.\n",
    "- Are there more false positives or false negatives? (remember we are trying to predict evergreen-ness)\n",
    "- How does that relate to what the `classification_report` is showing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted_evergreen  predicted_not_evergreen\n",
      "evergreen                     2033                      378\n",
      "not_evergreen                  779                     1720\n"
     ]
    }
   ],
   "source": [
    "conmat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['evergreen', 'not_evergreen'],\n",
    "                         columns=['predicted_evergreen','predicted_not_evergreen'])\n",
    "### i have no idae what my labels should be\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "# In order to do this, we \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.84      0.78      2411\n",
      "          1       0.82      0.69      0.75      2499\n",
      "\n",
      "avg / total       0.77      0.76      0.76      4910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving the model\n",
    "\n",
    "Can we improve the accuracy of the model?\n",
    "\n",
    "One way to do this is to use tune the parameters controlling it.\n",
    "\n",
    "You can get a list of all the model parameters using `model.get_params().keys()`.\n",
    "\n",
    "Discuss with your team which parameters you could try to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'LogisticRegression' has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-bcbcb3408386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'LogisticRegression' has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "LogisticRegression.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can systematically probe parameter combinations by using the `GridSearchCV` function. Implement a new classifier that searches the best parameter combination. (Remember that the 'CV' stands for 'cross-validation' so you don't need to use the train-test splits that you set up earlier.)\n",
    "\n",
    "1. How will you choose the grid granularity?\n",
    "1. How can you prevent the grid to exponentially grow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assess the tuned model\n",
    "\n",
    "A tuned grid search model stores the best parameter combination and the best estimator as attributes.\n",
    "\n",
    "1. Use these to generate a new prediction vector `y_pred`.\n",
    "- Use the `confusion matrix` and `classification_report` to assess the accuracy of the new model.\n",
    "- How does the new model compare with the old one?\n",
    "- What else could you do to improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "What would happen if we used a different scoring function? Would our results change?\n",
    "Choose one or two classification metrics from the [sklearn provided metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) and repeat the grid_search. Do your result change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 10px\">\n",
    "\n",
    "#  API Demo / Lab + NLP\n",
    "Week 8 | 3.3\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/RNAEgP.jpg\" width=\"600\">\n",
    "\n",
    "Can we correctly identify which of these two old men tweeted what?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 mins) Opening \n",
    "\n",
    "We are going to:\n",
    "\n",
    "- Create a developer account on Twitter\n",
    "- Create a method to pull a list of tweets from the Twitter API\n",
    "\n",
    "(Lab)\n",
    "- Perform proper preprocessing on our text\n",
    "- Engineer sentiment feature in our dataset using TextBlob\n",
    "- Explore supervised classification techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Developer Registration\n",
    "\n",
    "If you haven't registered a Twitter account yet, this is a requirement in order to have a \"developer\" account.\n",
    "\n",
    "[Twitter Rest API](https://dev.twitter.com/rest/public)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an \"App\"\n",
    "\n",
    "![](https://snag.gy/HPBQbJ.jpg)\n",
    "\n",
    "We now will now go to Twitter and register an \"app\" [apps.twitter.com](https://apps.twitter.com/).  After we set up our app, we will only need to reference the cooresponding keys Twitter generates for our app.  These are the keys that we will use with our application to communicate with the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python Twitter API library\n",
    "\n",
    "Someone was nice enough to build a nice libary for us in Python that we only need to plug in our keys and start collecting data with.  The library we will be using is provided by [Python Twitter Tools](http://mike.verdone.ca/twitter/).\n",
    "\n",
    "To install it, just run the next frame (there is no conda package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-twitter\n",
      "  Downloading python_twitter-3.2.1-py2-none-any.whl (71kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 391kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /anaconda/lib/python2.7/site-packages (from python-twitter)\n",
      "Collecting requests-oauthlib (from python-twitter)\n",
      "  Downloading requests_oauthlib-0.8.0-py2.py3-none-any.whl\n",
      "Collecting future (from python-twitter)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 491kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting oauthlib>=0.6.2 (from requests-oauthlib->python-twitter)\n",
      "  Downloading oauthlib-2.0.2.tar.gz (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 446kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, oauthlib\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/jennydoyle/Library/Caches/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "  Running setup.py bdist_wheel for oauthlib ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/jennydoyle/Library/Caches/pip/wheels/84/98/7a/fba7268f61097bea6081cbe5480bc439b38975748ea7684fd5\n",
      "Successfully built future oauthlib\n",
      "Installing collected packages: oauthlib, requests-oauthlib, future, python-twitter\n",
      "Successfully installed future-0.16.0 oauthlib-2.0.2 python-twitter-3.2.1 requests-oauthlib-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Boring Twitter Rules\n",
    "\n",
    "Twitter says they will rate limit your requests:\n",
    "\n",
    ">When using application-only authentication, rate limits are determined globally for the entire application. If a method allows for 15 requests per rate limit window, then it allows you to make 15 requests per window — on behalf of your application. This limit is considered completely separately from per-user limits. https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Here's a quick overview of what Twitter says are \"the rulez\":\n",
    "\n",
    "![](https://snag.gy/yJ6vIH.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About those Keys: OAuth Review\n",
    "\n",
    "![](https://g.twimg.com/dev/documentation/image/appauth_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going on here?  Take a minute.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Application Keys\n",
    "\n",
    "Take note of our application keys that we will be using with our little application that will be connecting to Twitter and mining Tweets from the official twitter accounts of our choosing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Miner Class Setup\n",
    "\n",
    "The following code is meant to get us up and running with connectivity to twitter, and the ability to make requests and easily transform the JSON responses to DataFrames.  We will be using object oriented Python in order to organize our code.  We may go into review since this was a topic we covered earlier in the class but we can review it during the lab for those who want to know more about it.\n",
    "\n",
    "\n",
    "> \"request_limit\" is used in this class to limit the number of tweets that are pulled per instance request.  Setting it to something lower until you've worked the bugs out of your request, and captured the data you want, is essential to avoiding any rate limit blocks.\n",
    "\n",
    "#### Key Setup\n",
    "\n",
    "- **consumer_key** - Find this in your app page under the \"Keys and Access Tokens\"\n",
    "- **consumer_secret** - Right under **consumer_key** in the \"Keys and Access Tokens\" tab\n",
    "- **access_token_key** - You will need to click the \"generate tokens\" button to get this\n",
    "- **access_token_secret** - Also available after \"generate tokens\" is pressed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, re, datetime, pandas as pd\n",
    "\n",
    "class twitterminer():\n",
    "\n",
    "    request_limit   =   20    \n",
    "    api             =   False\n",
    "    data            =   []\n",
    "    \n",
    "    twitter_keys = {\n",
    "        'consumer_key':        'PiTX0DzHlUrNuzoIA3o2QqZMZ',\n",
    "        'consumer_secret':     '268IRWLiu4QdUGVODmIyKEE2AjzdD5yUuEJhOCwg3kmBrQuHwv',\n",
    "        'access_token_key':    '860481430236868608-tA8bf2kkJ5xacH9mTs7fkGc75c6Rb3x',\n",
    "        'access_token_secret': '6b8mV0h1pZDJQTMornpJQNc3cXPpzSvE7QXRr4l6YLc6N'\n",
    "    }\n",
    "    \n",
    "    def __init__(self,  request_limit = 20):\n",
    "        \n",
    "        self.request_limit = request_limit\n",
    "        \n",
    "        # This sets the twitter API object for use internall within the class\n",
    "        self.set_api()\n",
    "        \n",
    "    def set_api(self):\n",
    "        \n",
    "        self.api = twitter.Api(\n",
    "            consumer_key         =   self.twitter_keys['consumer_key'],\n",
    "            consumer_secret      =   self.twitter_keys['consumer_secret'],\n",
    "            access_token_key     =   self.twitter_keys['access_token_key'],\n",
    "            access_token_secret  =   self.twitter_keys['access_token_secret']\n",
    "        )\n",
    "\n",
    "    def mine_user_tweets(self, user=\"dyerrington\", mine_retweets=False):\n",
    "\n",
    "        statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.request_limit)\n",
    "        data       =   []\n",
    "        \n",
    "        for item in statuses:\n",
    "\n",
    "            mined = {\n",
    "                'tweet_id': item.id,\n",
    "                'handle': item.user.name,\n",
    "                'retweet_count': item.retweet_count,\n",
    "                'text': item.text,\n",
    "                'mined_at': datetime.datetime.now(),\n",
    "                'created_at': item.created_at,\n",
    "            }\n",
    "            \n",
    "            data.append(mined)\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does anyone remember how we \"instantiate\" a new instance of this class?\n",
    "\n",
    "**Bonus bonus** How do we call the method to *mine_user_tweets()*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created_at': u'Sat Mar 25 05:31:15 +0000 2017',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712147),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'A new favorite: Deathforce - CD/MD: BlaqKemistry by The Deathforce https://t.co/TiYaLrJdLt on #SoundCloud',\n",
       "  'tweet_id': 845508355632349184},\n",
       " {'created_at': u'Wed May 18 05:47:02 +0000 2016',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712158),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'A new favorite: Pieces by Ragle Gumm https://t.co/DRd6sxHzru on #SoundCloud',\n",
       "  'tweet_id': 732809702866849792},\n",
       " {'created_at': u'Mon Dec 14 20:44:06 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712161),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'Our project was a featured winner!  Thanks to my colleges Asjed Hussain and Sri Kanajan for the hard work! https://t.co/onyUNdJVyb',\n",
       "  'tweet_id': 676502950856921088},\n",
       " {'created_at': u'Tue Sep 01 20:09:19 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712164),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'My @Quora answer to How I can upload a large dataset successively (~50 GB, which consists of 100K files) in order to\\u2026 http://t.co/yO1tKbjUUh',\n",
       "  'tweet_id': 638805859443806208},\n",
       " {'created_at': u'Tue Sep 01 18:44:54 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712166),\n",
       "  'retweet_count': 1,\n",
       "  'text': u'Initial commits for @livecodingtv chat-bots &amp; channel utils:  https://t.co/WpGYGWLK5m, https://t.co/qRIiO9pUxh, https://t.co/xhMrRdOI0q',\n",
       "  'tweet_id': 638784613293408257},\n",
       " {'created_at': u'Fri Aug 21 22:33:49 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712169),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'@itschriscates I think this book is awesome:  http://t.co/CoZ77Pg0eC, R vs Python: http://t.co/f0sNweD14M MySQL does work with R ;)',\n",
       "  'tweet_id': 634855956472426497},\n",
       " {'created_at': u'Fri Aug 21 22:29:42 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712172),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'@itschriscates yeah sure.  Point me at some data and we can look at it on the stream one night.  Monday we shall ride again!',\n",
       "  'tweet_id': 634854920382844928},\n",
       " {'created_at': u'Fri Aug 21 18:48:27 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712175),\n",
       "  'retweet_count': 1,\n",
       "  'text': u'@itschriscates thanks Chris!  Next time we will up the ante like sacrifice a virtualenv or catch rattle snakes.',\n",
       "  'tweet_id': 634799241362145281},\n",
       " {'created_at': u'Thu Aug 20 23:24:47 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712177),\n",
       "  'retweet_count': 102,\n",
       "  'text': u'RT @DataScienceCtrl: Cheat Sheet: Data Visualization with R http://t.co/4Vdm6HJxzW http://t.co/PSLaKJ4Jud',\n",
       "  'tweet_id': 634506392674672640},\n",
       " {'created_at': u'Thu Aug 20 20:17:44 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712180),\n",
       "  'retweet_count': 2,\n",
       "  'text': u'What is it like to interview as a data scientist? #datascience #datascientists  #machinelearning https://t.co/DLLzBYdotB',\n",
       "  'tweet_id': 634459320759939072},\n",
       " {'created_at': u'Sat Jul 11 19:11:18 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712183),\n",
       "  'retweet_count': 0,\n",
       "  'text': u\"@ChiedoJohn If this looks interesting, I'm open to an dev shop taking it over potentially:  http://t.co/FcHCXYBfi4\",\n",
       "  'tweet_id': 619947087652745216},\n",
       " {'created_at': u'Sat Jul 11 19:03:51 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712185),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'@ChiedoJohn Glad to hear.  Want to work with it more?',\n",
       "  'tweet_id': 619945213843275777},\n",
       " {'created_at': u'Fri Jun 12 20:04:33 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712188),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'Probability dist explained simply with Naive Bayes in NLP problems. http://t.co/J8cLl6y9Tx',\n",
       "  'tweet_id': 609451241211592704},\n",
       " {'created_at': u'Tue May 19 21:45:52 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712191),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'A new favorite: killer mike - 2 sides (bootleg) by @Lindsay_Lowend https://t.co/04RiT5U6z9 on #SoundCloud',\n",
       "  'tweet_id': 600779430517153792},\n",
       " {'created_at': u'Thu May 14 21:41:35 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712193),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'@BerkeleyBowl Why do I keep buying bad avocados? http://t.co/rdxcZ0X1dg',\n",
       "  'tweet_id': 598966413894778880},\n",
       " {'created_at': u'Wed May 13 16:10:42 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712195),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'Topic modeling with LDA: MLlib meets GraphX http://t.co/Ipv3Kvy2gO',\n",
       "  'tweet_id': 598520754435727360},\n",
       " {'created_at': u'Thu Apr 30 15:45:12 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712198),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'First BigObject benchmark looks Amazing http://t.co/7v8MBbcW58',\n",
       "  'tweet_id': 593803296827117568},\n",
       " {'created_at': u'Wed Apr 29 15:55:12 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712201),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'Improper applications of Principal Component Analysis on multimodal data http://t.co/Fpr4cLcGXM',\n",
       "  'tweet_id': 593443425376149505},\n",
       " {'created_at': u'Tue Apr 28 15:45:16 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712203),\n",
       "  'retweet_count': 1,\n",
       "  'text': u'Math of Ideas: A Word is Worth a Thousand Vectors http://t.co/8CiqOhuSd0',\n",
       "  'tweet_id': 593078535180390402},\n",
       " {'created_at': u'Tue Apr 07 18:40:20 +0000 2015',\n",
       "  'handle': u'David Yerrington',\n",
       "  'mined_at': datetime.datetime(2017, 5, 5, 11, 52, 51, 712206),\n",
       "  'retweet_count': 0,\n",
       "  'text': u'Zargoza 2012, #phototravel.  Even in the middle of nowhere Zargoza Spain, you can find jamon Iberico and Serrano. http://t.co/V2kRxbaGpy',\n",
       "  'tweet_id': 585512447991226368}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional twitter ids:  realDonaldTrump, berniesanders\n",
    "# Let's test this out here..\n",
    "\n",
    "twitterminer()\n",
    "twitterminer().mine_user_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Now we create some training data\n",
    "\n",
    "We will have to munge a little bit in order to get our \"mined\" data from the Twitter API.  \n",
    "\n",
    " - Mine User1 Tweets\n",
    " - Create DataFrame\n",
    " - Mine User2 Tweets\n",
    " - Append to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we only need to \"instantiate\" once.  Then we can call mine_user_tweets as much as we want.\n",
    "miner = twitterminer(request_limit=100)\n",
    "\n",
    "# insert handle we like\n",
    "trump_tweets = miner.mine_user_tweets(\"realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri May 05 13:02:46 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818801</td>\n",
       "      <td>5721</td>\n",
       "      <td>Rather than causing a big disruption in N.Y.C....</td>\n",
       "      <td>860479885566980096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri May 05 12:52:36 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818819</td>\n",
       "      <td>5069</td>\n",
       "      <td>Big win in the House - very exciting! But when...</td>\n",
       "      <td>860477328882905089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri May 05 06:16:06 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818826</td>\n",
       "      <td>4860</td>\n",
       "      <td>An honor to welcome PM of Australia, @Turnbull...</td>\n",
       "      <td>860377543878348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri May 05 02:55:02 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818831</td>\n",
       "      <td>9927</td>\n",
       "      <td>It was a GREAT day for the United States of Am...</td>\n",
       "      <td>860326946147840001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu May 04 18:07:29 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818835</td>\n",
       "      <td>7757</td>\n",
       "      <td>If victorious, Republicans will be having a bi...</td>\n",
       "      <td>860194183797243904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thu May 04 17:56:51 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818839</td>\n",
       "      <td>11253</td>\n",
       "      <td>Insurance companies are fleeing ObamaCare - it...</td>\n",
       "      <td>860191508619722752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thu May 04 17:43:11 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818844</td>\n",
       "      <td>13084</td>\n",
       "      <td>I am watching the Democrats trying to defend t...</td>\n",
       "      <td>860188065758838785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thu May 04 15:26:00 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818847</td>\n",
       "      <td>7265</td>\n",
       "      <td>Beautiful evening with Religious Leaders here ...</td>\n",
       "      <td>860153543151542272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thu May 04 12:28:39 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818852</td>\n",
       "      <td>7548</td>\n",
       "      <td>Death spiral!\\n'Aetna will exit Obamacare mark...</td>\n",
       "      <td>860108914624532480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thu May 04 11:09:26 +0000 2017</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2017-05-05 12:02:58.818859</td>\n",
       "      <td>9490</td>\n",
       "      <td>RT @foxandfriends: President Trump to sign an ...</td>\n",
       "      <td>860088977277235201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at           handle                   mined_at  \\\n",
       "0  Fri May 05 13:02:46 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818801   \n",
       "1  Fri May 05 12:52:36 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818819   \n",
       "2  Fri May 05 06:16:06 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818826   \n",
       "3  Fri May 05 02:55:02 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818831   \n",
       "4  Thu May 04 18:07:29 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818835   \n",
       "5  Thu May 04 17:56:51 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818839   \n",
       "6  Thu May 04 17:43:11 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818844   \n",
       "7  Thu May 04 15:26:00 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818847   \n",
       "8  Thu May 04 12:28:39 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818852   \n",
       "9  Thu May 04 11:09:26 +0000 2017  Donald J. Trump 2017-05-05 12:02:58.818859   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0           5721  Rather than causing a big disruption in N.Y.C....   \n",
       "1           5069  Big win in the House - very exciting! But when...   \n",
       "2           4860  An honor to welcome PM of Australia, @Turnbull...   \n",
       "3           9927  It was a GREAT day for the United States of Am...   \n",
       "4           7757  If victorious, Republicans will be having a bi...   \n",
       "5          11253  Insurance companies are fleeing ObamaCare - it...   \n",
       "6          13084  I am watching the Democrats trying to defend t...   \n",
       "7           7265  Beautiful evening with Religious Leaders here ...   \n",
       "8           7548  Death spiral!\\n'Aetna will exit Obamacare mark...   \n",
       "9           9490  RT @foxandfriends: President Trump to sign an ...   \n",
       "\n",
       "             tweet_id  \n",
       "0  860479885566980096  \n",
       "1  860477328882905089  \n",
       "2  860377543878348800  \n",
       "3  860326946147840001  \n",
       "4  860194183797243904  \n",
       "5  860191508619722752  \n",
       "6  860188065758838785  \n",
       "7  860153543151542272  \n",
       "8  860108914624532480  \n",
       "9  860088977277235201  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name our df aptly\n",
    "trump_df = pd.DataFrame(trump_tweets)\n",
    "trump_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any interesting ngrams going on with Trump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'https co', 36),\n",
       " (u'at the', 9),\n",
       " (u'executive order', 8),\n",
       " (u'will be', 8),\n",
       " (u'of the', 7),\n",
       " (u'honor to', 6),\n",
       " (u'in the', 6),\n",
       " (u'insurance companies', 6),\n",
       " (u'the democrats', 6),\n",
       " (u'order on', 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "summaries = \"\".join(trump_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 mins) Try this exercize with Bernie Sanders.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sanders_tweets = miner.mine_user_tweets(\"berniesanders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = pd.DataFrame(trump_tweets + sanders_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing our Tweets\n",
    "\n",
    "In order to do classfication recall that we need a set of features.  Our features are literally what our presidential hopefulls say on Twitter. \n",
    "\n",
    "We will need to:\n",
    "- Vectorize input text data\n",
    "- Intialize a model (let's try Logistic regression)\n",
    "- Train / Predict / Cross Validate\n",
    "- Score / Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Preprocess our text data to Tfidf\n",
    "tfv = TfidfVectorizer(lowercase=True, strip_accents='unicode')\n",
    "X_all = tfv.fit_transform(all_tweets['text'])\n",
    "\n",
    "# Setup logistic regression (or try another classification method here)\n",
    "estimator = LogisticRegression()\n",
    "estimator.fit(X_all, all_tweets['handle'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Prediction vs Random Sanders Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63681826,  0.36318174],\n",
       "       [ 0.29723894,  0.70276106]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep our source as TfIdf vectors\n",
    "source_test = [\n",
    "    \"Demanding that the wealthy and the powerful start paying their fair share of taxes that's exactly what the American people want.\",\n",
    "    \"Crooked Hillary is spending tremendous amounts of Wall Street money on false ads against me. She is a very dishonest person!\"\n",
    "]\n",
    "\n",
    "############\n",
    "# NOTE:  Do not re-initialize the tfidf vectorizor or the feature space willbe overwritten and\n",
    "# hence your transform will not match the number of features you trained your model on.\n",
    "#\n",
    "# This is why you only need to \"transform\" since you already \"fit\" previously\n",
    "#\n",
    "####\n",
    "\n",
    "X_all = tfv.transform(source_test)\n",
    "\n",
    "# Predict using previously trained logist regression `estimator`\n",
    "estimator.predict_proba(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Time\n",
    "\n",
    "### The following is available in the lab folder as well -- please do your work in _that_ notebook.\n",
    "\n",
    "We would like you to perform an analysis using a proper cross validation.  Also, try classfication using other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the same analysis using more data.\n",
    "\n",
    "Experiment with using more data.  The API may not like that you are blowing through their limits so definitely be careful.  Try to grab only what you need 1x, then work on the copy of the objects that are returned.  Read the documents about rate limits and see if you can get enough without hitting the rate limit.  Are there any options availabl in the API to avoid such a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement K-Folds or test/train split.\n",
    "\n",
    "Double check that you are getting random data before moving forward.  What would happen if you over sample Trump more than Sanders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mine more Tweets that aren't in your data set\n",
    "Or use the hold-out method to do a proper test.  Refer back to our advanced classification evaluation lesson if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check your classification report\n",
    "How's precision / recall of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Change out your TFIDF vectorizer for CountVectorizer.\n",
    "How has this impacted your mode performance at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Implement a different classification method such as random forrests.\n",
    "Or pick one of your favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Try to remove stopwords from your text during your preprocessing step\n",
    "\n",
    "Then double check your classfication report.  Have things improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Try removing samples that have links or that are obviously just announcements or \"noise\" that doesn't appear to represent \"True\" tweets by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What are some contrasting words or phrases that you can see between the ngrams for each author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  What do you think you can do to improve the scores further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. **BONUS** Using TextBlob, add a sentiment feature to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. BONUS BONUS Apply PCA to your text features\n",
    "Is this effective? (ie: we could talk about LDA here a little bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "- What where the most impactful changes that helped your models?\n",
    "- What do you think would happen if we had more user1 Tweets than user2?\n",
    "- What other projects might you think to apply these problems against?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

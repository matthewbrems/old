{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime Library\n",
    "Fun story time! Pandas started out in the financial world, which is why it's so great at everything related to timeseries.\n",
    "\n",
    "Today, we're going to review datetime objects, look at timedeltas, generate basic timeseries plots, and calculate autocorrelation using python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The date time library is something you should already have because of Anaconda.\n",
    "from datetime import datetime\n",
    "# And quite a few of you are already familiar with\n",
    "\n",
    "# Let's look at the date we once believed the world would end on.\n",
    "lesson_date = datetime(2012, 12, 21, 12, 21, 12, 844089)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Micro-Second\", lesson_date.microsecond\n",
    "print \"Second\", lesson_date.second\n",
    "print \"Minute\", lesson_date.minute\n",
    "print \"Hour\", lesson_date.hour\n",
    "print \"Day\", lesson_date.day\n",
    "print \"Month\",lesson_date.month\n",
    "print \"Year\", lesson_date.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timedelta\n",
    "Say we want to add or subtract time to/from a date. Perhaps we're using time as an index and we want to get everything that happened a week before a specific observation, for example.\n",
    "\n",
    "We can use a timedelta object to shift a Datetime object. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import timedelta from datetime library\n",
    "from datetime import timedelta\n",
    "\n",
    "# Time deltas represent time as an amount as opposed to a fixed position.\n",
    "offset = timedelta(days=1, seconds=20)\n",
    "\n",
    "# the time delta has attributes that allow us to extract values from it.\n",
    "print 'offset days', offset.days\n",
    "print 'offset seconds', offset.seconds\n",
    "print 'offset microseconds', offset.microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print \"It's now: \", now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Future: \", now + offset\n",
    "print \"Past: \", now - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The largest value a time delta can hold is 'Days'.  I.e. you can't say you want you an offset to be 2 years, 44 days and 12 hours.  You would have to manually convert the time of those years to be represented in days._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a dataset from the internets\n",
    "import pandas as pd\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see that the Time column is just an object.\n",
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Overwrite the original Time column with one that has been converted to a datetime series.\n",
    "ufo['Time'] = pd.to_datetime(ufo.Time)\n",
    "\n",
    "#Letting pandas guess how to do this can take a little bit of time we can use a few arguments to help.\n",
    "'''ufo['Time'] = pd.to_datetime(ufo.Time, format='%Y%m%d', errors='coerce')'''\n",
    "# Format will let pandas know what format pandas should use to interpret the date as\n",
    "# errors will allow you to automatically deal with errors when converting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the time column looks a bit different now!\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's take a look at how the series has changed\n",
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also use dt to get weekday names \n",
    "ufo.Time.dt.weekday_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#and what day of the year it was!\n",
    "ufo.Time.dt.dayofyear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent activity:\n",
    "Take 10 minutes to look at the different ways you can work with timezones and timezone formatting. Try creating a few new columns for things like daylight savings adjustment, timezone name, etc.\n",
    "\n",
    "https://docs.python.org/2/library/datetime.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's create a timestamp of interest\n",
    "ts = pd.to_datetime('9/10/1993')\n",
    "#^that's the day x-files first came out, for all of you wondering\n",
    "ts\n",
    "# The main difference between a Datetime object and a timestamp is...\n",
    "# that timestamps can be used as comparisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the timestamp we just saved to create a new dataframe.\n",
    "ufo.loc[ufo.Time >= ts, :].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we could create a new column looking at how far away from our point of interest a particular UFO was sighted\n",
    "ufo['new'] = ufo.Time - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Timedelta can also be used to get the min and max of a timeseries.\n",
    "ufo.Time.max() - ufo.Time.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also use timedelta to mess around with the silly YouTube videos you're embedding in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"hAAlDoAtV7Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start=int(timedelta(minutes=1, seconds=2).total_seconds())\n",
    "YouTubeVideo(\"hAAlDoAtV7Y\", start=start, autoplay=1, theme=\"light\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More independent work: \n",
    "\n",
    "Search for .dt. on http://pandas.pydata.org/pandas-docs/stable/api.html for more information about pandas Datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a timeseries using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's load in a different dataset\n",
    "crime = pd.read_csv('https://raw.githubusercontent.com/rufuspollock/crime-data-sf/gh-pages/data/sfpd_incidents_march_2012.tidied.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#taking a look at our different types\n",
    "crime.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#so do we want to mess around with the date or the time?\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's turn date into a datetime object\n",
    "crime['Date'] = pd.to_datetime(crime.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I'm arbitrarily picking weekday to be how we look at our data\n",
    "crime['weekday'] = crime.Date.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's groupby weekday on this \n",
    "crime_ts = crime.groupby('weekday').aggregate(len)['IncidntNum']\n",
    "#the groupby statement automatically makes weekday the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(crime_ts.index, crime_ts.values, lw=5)\n",
    "#LW = line width!\n",
    "#a small stringed instrument! a classical timeseries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's convert the date to be the index\n",
    "crime.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime['Month'] = crime.index.month\n",
    "crime['weekday'] = crime.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#an FYI-- filtering by date becomes really easy when you're working with it as an index!\n",
    "crime['2012-03-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#including looking at a range of observations\n",
    "crime['3/3/2012':'3/4/2012']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick intro to autocorrelation and window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data!\n",
    "url = 'https://raw.githubusercontent.com/sinanuozdemir/sfdat22/master/data/rossmann.csv'\n",
    "data = pd.read_csv(url, skipinitialspace=True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most interested in date - format properly and convert to index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new columns for year and month \n",
    "data['Year'] = data.index.year\n",
    "data['Month'] = data.index.month\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are over a million sales data points in this dataset, so for some simple EDA we will focus on just one store.\n",
    "store1_data = data[data.Store == 1]\n",
    "store1_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As we begin to study the sales from this drugstore, we also want to know both the time dependent elements of sales as \n",
    "well as whether promotions or holidays effected these sales. To start, we can compare the average sales on those events.\n",
    "To compare sales on holidays, we can compare the sales using box-plots, which allows us to compare the distribution of \n",
    "sales on holidays against all other days. On state holidays the store is closed (which means there are 0 sales), and \n",
    "on school holidays the sales are relatively similar. These types of insights represent the contextual knowledge needed \n",
    "to truly explain time series phenomenon. Can you think of any other special considerations we should make when tracking sales?\n",
    "'''\n",
    "\n",
    "# check similarity between School Holiday and Sales\n",
    "sns.factorplot(\n",
    "    x='SchoolHoliday',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  We can see that there is a difference in sales on promotion days\n",
    "sns.factorplot(\n",
    "    col='Open',\n",
    "    x='Promo',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Why is it important to separate out days where the store is closed? \n",
    "Because there aren't any promotions on those days either, so including \n",
    "them will bias your sales data on days without promotions! Remember: \n",
    "Data Scientists needs to think about the business logic (context) as well as \n",
    "analyzing the raw data.\n",
    "'''\n",
    "\n",
    "# perhaps plot sales across day of the week\n",
    "sns.factorplot(\n",
    "    col='Open',\n",
    "    x='DayOfWeek',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Consider sales across multiple years. How did sales change from 2014 to 2015?\n",
    "\n",
    "# Filter to days store 1 was open\n",
    "store1_open_data = store1_data[store1_data.Open==1]\n",
    "store1_open_data[['Sales']].plot()          # sales over time\n",
    "store1_open_data[['Customers']].plot()      # customers over time\n",
    "\n",
    "# EXERCISE: Use filtering to show the trend in 2015 alone\n",
    "\n",
    "store1_data_2015 = store1_data['2015']\n",
    "store1_data_2015[\n",
    "    store1_data_2015.Open==1\n",
    "][['Sales']].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check:\n",
    "\n",
    "What is autocorrelation?\n",
    "\n",
    "Autocorrelation features measure the statistical correlation of a time series with a _lagged_ version of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Computing Autocorrelation\n",
    "To measure how much the sales are correlated with each other, we want to compute \n",
    "the autocorrelation of the 'Sales' column. In pandas, we'll do this with the \n",
    "autocorr function.\n",
    "autocorr takes one argument, the lag - which is how many prior data points \n",
    "should be used to compute the correlation. If we set the lag to 1, we compute \n",
    "the correlation between every point and the point directly preceding it, \n",
    "If we set lag to 10, this computes the correlation between every point \n",
    "and the point 10 days earlier:\n",
    "'''\n",
    "\n",
    "data['Sales'].resample('D').mean().autocorr(lag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#that's a pretty small mean correlation. what if we look at the autocorrelation for 30 days\n",
    "data['Sales'].resample('D').mean().autocorr(lag=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "If we want to investigate trends over time in sales, as always, we will \n",
    "start by computing simple aggregates. We want to know: what were the mean \n",
    "and median sales in each month and year?\n",
    "In Pandas, this is performed using the resample command, which is very \n",
    "similar to the groupby command. It allows us to group over different \n",
    "time intervals.\n",
    "We can use data.resample and provide as arguments: - The level on \n",
    "which to roll-up to, 'D' for day, 'W' for week, 'M' for month, 'A' \n",
    "for year - The aggregation to perform: 'mean', 'median', 'sum', etc.\n",
    "'''\n",
    "\n",
    "# Here we can see again that December 2013 and 2014 were the highest average sale months.\n",
    "data[['Sales']].resample('A').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.resample('A').mean()    # whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[['Sales']].resample('M').mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resample to have the daily total over all stores\n",
    "# Alternatively, this could a daily average over all store with how='mean'\n",
    "daily_store_sales = data[['Sales']].resample('D').mean()\n",
    "daily_store_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CHECK: What is a rolling mean? Why might it be useful?\n",
    "\n",
    "# 3-day rolling mean of daily store sales\n",
    "pd.rolling_mean(daily_store_sales, window=3, center=True)\n",
    "pd.rolling_mean(daily_store_sales, window=3, center=True)['2015']   # filter to 2015 only\n",
    "pd.rolling_mean(daily_store_sales, window=10, center=True).plot()   # plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also use exponential moving average. CHECK: What is the difference?\n",
    "pd.ewma(data['Sales'], span=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "WINDOW FUNCTIONS\n",
    "Pandas rolling_mean and rolling_median are only two examples of Pandas\n",
    "window function capabilities. Window functions operate on a set of N\n",
    "consecutive rows (i.e.: a window) and produce an output.\n",
    "n addition to rolling_mean and rolling_median, there are rolling_sum,\n",
    "rolling_min, rolling_max... and many more.\n",
    "Another common one is diff, which takes the difference over time.\n",
    "pd.diff takes one argument: periods, which measures how many rows\n",
    "prior to use for the difference.\n",
    "For example, if we want to compute the difference in sales,\n",
    "day by day, we could compute:\n",
    "'''\n",
    "\n",
    "daily_store_sales.diff(periods=1) # day by day difference in sales\n",
    "daily_store_sales.diff(periods=7) # compare same day each week\n",
    "\n",
    "# Difference functions allow us to identify seasonal changes when we see repeated up or downswings.\n",
    "# An example from FiveThirtyEight:\n",
    "# http://i2.wp.com/espnfivethirtyeight.files.wordpress.com/2015/03/casselman-datalab-wsj2.png?quality=90&strip=all&w=575&ssl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Pandas Expanding Functions\n",
    "In addition to the set of rolling_* functions, Pandas also \n",
    "provides a similar collection of expanding_* functions, which, \n",
    "instead of using a window of N values, uses all values up until \n",
    "that time.\n",
    "'''\n",
    "\n",
    "\n",
    "pd.expanding_mean(daily_store_sales) # average date from first till last date specified\n",
    "pd.expanding_sum(daily_store_sales) # sum of average sales per store until that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXERCISES\n",
    "1. Plot the distribution of sales by month and compare the effect of promotions.\n",
    "hint: try using hue in sns\n",
    "2. Are sales more correlated with the prior date, a similar date last year, or a similar date last month?\n",
    "4. Identify the date with largest drop in sales from the same date in the previous week.\n",
    "5. Compute the total sales up until Dec. 2014.\n",
    "6. When were the largest differences between 15-day moving/rolling averages? HINT: Using rolling_mean and diff\n",
    "'''\n",
    "\n",
    "# Plot the distribution of sales by month and compare the effect of promotions\n",
    "sns.factorplot(\n",
    "    col='Open',\n",
    "    hue='Promo',\n",
    "    x='Month',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Are sales more correlated with the prior date, a similar date last year, or a similar date last month?\n",
    "# Compare the following:\n",
    "average_daily_sales = data[['Sales', 'Open']].resample('D', how='mean')\n",
    "\n",
    "print average_daily_sales['Sales'].autocorr(lag=1)        # day\n",
    "\n",
    "print average_daily_sales['Sales'].autocorr(lag=30)       # month  \n",
    "\n",
    "average_daily_sales['Sales'].autocorr(lag=365)      # year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify the date with largest drop in average store sales from the same date in the previous month:\n",
    "average_daily_sales = data[['Sales', 'Open']].resample('D', how='mean')\n",
    "average_daily_sales['DiffVsLastWeek'] = average_daily_sales[['Sales']].diff(periods=7)\n",
    "\n",
    "average_daily_sales.sort(['DiffVsLastWeek']).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unsurprisingly, this day is Dec. 25 and Dec. 26 in 2014 and 2015, when the store is closed and there are many sales in the preceding week. How about when the store is open?\n",
    "average_daily_sales[average_daily_sales.Open == 1].sort(['DiffVsLastWeek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the total sales up until Dec. 2014:\n",
    "total_daily_sales = data[['Sales']].resample('D', how='sum')\n",
    "pd.expanding_sum(total_daily_sales)['2014-12']\n",
    "# THIS IS NOT pd.expanding_sum(data['Sales'])['2014-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When were the largest differences between 15-day moving/rolling averages? HINT: Using rolling_mean and diff\n",
    "pd.rolling_mean(total_daily_sales, window=15).diff(1).sort('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming note on using time series for Capstones:\n",
    "\n",
    "Here's an example of using timeseries for a Capstone: https://github.com/samuel-stack/Portfolio/blob/master/Moving%20Violations%20VS.%20Speed%20Traps/Granger%20Causality%20test%20.ipynb\n",
    "\n",
    "Note, this Capstone makes use of Granger Causality: a statistical concept that says if a signal X \"Granger-causes\" (or \"G-causes\") a signal Y, then past values of X should contain information that helps predict Y above and beyond the information contained in past values of Y alone. \n",
    "\n",
    "To put it another way, a time series X1 is said to Granger-cause Y if the X1 values provide statistically significant information about future values of Y. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

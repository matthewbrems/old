{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction: 20 Questions\n",
    "\n",
    "In the game \"20 Questions,\" an individual or group has the opportunity to identify an unknown object by asking a series of up to twenty \"yes\" or \"no\" questions.\n",
    "\n",
    "For example, suppose that I am trying to guess the item \"marker.\" I might ask a series of questions like:\n",
    "- Is the object in this room? (Yes.)\n",
    "- Is the object within five feet of me? (Yes.)\n",
    "- Is the object larger than a loaf of bread? (No.)\n",
    "- Is there only one of these objects in the room? (No.)\n",
    "- Do you hold this object when you use it? (Yes.)\n",
    "- Is the object a pen? (No.)\n",
    "- Is the object a book? (No.)\n",
    "- Is the object a marker? (Yes!)\n",
    "\n",
    "We can think about all possible \"target objects\" and then ask questions that help us to quickly pare down the number of objects so that we can identify the true target in twenty of fewer questions.\n",
    "\n",
    "#### When playing this game, what are some good strategies? What about poor strategies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification and regression trees (CARTs)\n",
    "\n",
    "Decision trees are a widely popular and powerful machine learning technique for both classification and regression problems.\n",
    "\n",
    "To perform classification or regression, decision trees make sequential, hierarchical decisions about the outcome variable based on the predictor data.\n",
    "\n",
    "---\n",
    "\n",
    "[Classification CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "[Regression CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "\n",
    "[Decision tree user guide](http://scikit-learn.org/stable/modules/tree.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "b = datasets.load_boston()\n",
    "X, Y = b['data'], b['target']\n",
    "\n",
    "# criterion: splitting decision function for classification can be 'gini' or 'entropy'\n",
    "# criterion: splitting decision function for regression can be MSE or MAE\n",
    "# max_depth: the maximum number of hierarchical decision nodes (how \"deep\" the tree is built)\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='gini',\n",
    "                                    max_depth=None)\n",
    "\n",
    "regressor = DecisionTreeRegressor(criterion='mse',\n",
    "                                  max_depth=5)\n",
    "\n",
    "regressor.fit(X, Y)\n",
    "\n",
    "Y_pred = regressor.predict(X)\n",
    "\n",
    "# decision trees can give us feature importances. the higher the number the more important\n",
    "# the predictor was to deciding splits at nodes.\n",
    "# \"The importance of a feature is computed as the (normalized) total reduction of the \n",
    "# criterion brought by that feature.\"\n",
    "\n",
    "feature_importances = regressor.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of decision trees\n",
    "\n",
    "Decision tree models are **hierarchical** and **non-parametric**.\n",
    "\n",
    "**Hierarchical** means that the model is definied by a sequence of questions which yield a class label or value when applied to any observation. Once trained, the model behaves like a recipe, a series of \"if this then that\" conditions that yields a specific result for our input data.\n",
    "\n",
    "[**Non-parametric** methods](https://en.wikipedia.org/wiki/Nonparametric_statistics) stand in contrast to models like logistic regression or ordinary least squares regression. There are no underlying assumptions about the distribution of the data or the errors. Non-parametric models essentially start with no _assumed_ parameters about the data and construct them based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example decision tree: should I play golf?\n",
    "\n",
    "---\n",
    "\n",
    "![deciding to play golf](./images/golf-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Directed Acyclic Graphs (DAG)\n",
    "\n",
    "CART models are in fact a special case of [Directed Acyclic Graphs (DAG).](https://en.wikipedia.org/wiki/Directed_acyclic_graph) \n",
    "\n",
    "DAGs have **nodes** and **edges**. In the golf example above, the nodes represent the decision points about the output variable given the predictors, and the edges are the \"paths\" between nodes that represent answers to the questions.\n",
    "\n",
    "The **acyclic** part of DAGs means that the edges do not cycle back on themselves.\n",
    "\n",
    "- The top node is called the **root node**. It has 0 incoming edges, and 2+ outgoing edges. \n",
    "- Internal nodes test a condition on a specific feature. They have 1 incoming edge, and 2+ outgoing edges. \n",
    "- A **leaf node** contains a class label (or regression value). It has 1 incoming edge and 0 outgoing edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a decision tree\n",
    "\n",
    "Building decision trees requires algorithms capable of determining an optimal choice at each node. \n",
    "\n",
    "One such algorithm is [**Hunt's algorithm**](http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/decisionTree.html). This is a greedy, recursive algorithm that leads to a local optimum:\n",
    "\n",
    "- [**Greedy:**](https://en.wikipedia.org/wiki/Greedy_algorithm) the algorithm makes the most optimal decision it can at each step.\n",
    "- [**Recursive:**](https://en.wikipedia.org/wiki/Recursion) the algorithm splits task into subtasks and solves each the same way.\n",
    "- [**Local optimum:**](https://en.wikipedia.org/wiki/Local_optimum) the algorithm finds a solution just for the given neighborhood of points.\n",
    "\n",
    "The algorithm works by recursively partitioning records into smaller and smaller subsets. The partitioning decision is made at each node according to a metric called **purity.** A node is said to be 100% pure when all of its records belong to a single class (or have the same value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pseudocode classification decision tree algorithm\n",
    "\n",
    "---\n",
    "\n",
    "    Given a set of records Dt at node t:\n",
    "        If all records in Dt belong to class A: \n",
    "            t is a leaf node corresponding to class (Base case)\n",
    "        Else if Dt contains records from both A and B:\n",
    "            Create test condition to partition the observations\n",
    "            Define t as an internal node, with outgoing edges to child nodes\n",
    "            partition records in Dt with conditional test logic to child nodes\n",
    "            Recursively apply steps at each child node.\n",
    "\n",
    "- Splits can be binary way or multi-way. \n",
    "- Features can be categorical or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-way splits\n",
    "\n",
    "---\n",
    "\n",
    "![multi-way](./images/multi-way.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous measure decisions (regression trees)\n",
    "\n",
    "---\n",
    "\n",
    "![continuous trees](./images/Continuous-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization and \"purity\"\n",
    "\n",
    "Recall from the algorithm above we iteratively create test conditions to split the data. \n",
    "\n",
    "---\n",
    "\n",
    "In a binary classification task with classes 0 and 1, we say that a node has maximum purity when only one class is present:\n",
    "\n",
    "### $$ p(0|t) = 1 $$ or $$ p(1|t) = 1 $$\n",
    "\n",
    "In a binary classification task, a maximum impurity partition is given by the distribution (classification):\n",
    "\n",
    "### $$ p(0|t) = p(1|t) = 0.5 $$\n",
    "\n",
    "where both classes are equally present at node $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Purity objective function\n",
    "\n",
    "To achieve maximum purity we need an **objective function** to optimize. \n",
    "\n",
    "We want our objective function to measure the **gain in purity** from a particular split. Therefore it depends on the class distribution over the nodes (before and after the split). \n",
    "\n",
    "For example, let \n",
    "\n",
    "### $$p(i|t)$$ \n",
    "\n",
    "be the probability of **`class i`** in the data at **`node t`** (e.g., the fraction of records labeled **`i`** at node **`t`**) \n",
    "\n",
    "We then define an impurity function that will smoothly vary between the two extreme cases of minimum impurity (one class or the other only) and the maximum impurity case as an equal mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common purity functions (classification)\n",
    "\n",
    "---\n",
    "\n",
    "## $$ \\text{Entropy} = - \\sum_{i=1}^{classes} p(i\\;|\\;t)\\text{log}_2\\;p(i\\;|\\;t) $$\n",
    "\n",
    "## $$ \\text{Gini} = 1 - \\sum_{i=1}^{classes} p(i\\;|\\;t)^2 $$\n",
    "\n",
    "The Gini inpurity is primarily used in the CART algorithm, but both Gini and Entropy are available in sklearn's classification and decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![purity functions](./images/measures.png)\n",
    "\n",
    "---\n",
    "\n",
    "Impurity measures on their own they are not enough to tell us how a split will do. We need to look at impurity **before & after** the split. We can make this comparison using what is called the **gain**: \n",
    "\n",
    "## $$ \\Delta = I(\\text{parent}) - \\sum_{\\text{children}}\\frac{N_j}{N}I(\\text{child}_j) $$\n",
    "\n",
    "Where $I$ is the impurity measure, $N_j$ denotes the number of records at child node $j$, and $N$ denotes the number of records at the parent node. When $I$ is the [**entropy function**](https://en.wikipedia.org/wiki/Binary_entropy_function), this quantity is called the [**information gain**](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees).\n",
    "\n",
    "**[Nice example of how misclassification error can break branching.](http://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"figure\", figsize=(9, 7))\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We will make some data for these functions of 100 points of x from 0.1 to 10:\n",
    "# \n",
    "# 1. The general log function for x\n",
    "# 2. The sin function\n",
    "# 3. The exponential function\n",
    "# 4. The x**2 function\n",
    "\n",
    "def make_data(points=100):\n",
    "    x = np.linspace(0.1, 10, points)\n",
    "    \n",
    "    data = {\n",
    "        'x':x,\n",
    "        'ygenlog':stats.genlogistic.pdf(x, 20)*500 + np.random.normal(0, 2, size=points),\n",
    "        'ysin':np.sin(x)*20 + np.random.normal(0, 2, size=points),\n",
    "        'yexp':np.exp(x/1.3) + np.random.normal(0, 2, size=points),\n",
    "        'ysq':(x-5)**2 + np.random.normal(0, 1, size=points)\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "# The data is put into a dictionary for convenience:\n",
    "d = make_data()\n",
    "x = d['x']\n",
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "sns.regplot(x=x, y=d['ygenlog'], scatter_kws={'s':70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "sns.regplot(x=x, y=d['ysin'], scatter_kws={'s':70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "sns.regplot(x=x, y=d['yexp'], scatter_kws={'s':70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "sns.regplot(x=x, y=d['ysq'], scatter_kws={'s':70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This function builds trees for an x and y predictor and dependent variable from depth 1 to 10.\n",
    "# (Regression trees)\n",
    "# Tree predictions are put into a dictionary:\n",
    "\n",
    "def build_tree_models(x, y, tree_depth_range=range(1,11)):\n",
    "    tree_model_ypreds = {}\n",
    "    \n",
    "    for i in tree_depth_range:\n",
    "        model = DecisionTreeRegressor(max_depth=i)\n",
    "        model.fit(x[:, np.newaxis], y)\n",
    "        tree_model_ypreds[i] = model.predict(x[:, np.newaxis])\n",
    "        \n",
    "    return tree_model_ypreds\n",
    "\n",
    "# A function to iterate through our data dictionary of different functions and build the tree models:\n",
    "\n",
    "def fit_trees(data_dict):\n",
    "    trees_dict = {}\n",
    "    x = data_dict['x']\n",
    "    for label, ys in data_dict.items():\n",
    "        if not label == 'x':\n",
    "            tree_ys = build_tree_models(x, ys)\n",
    "            trees_dict[label] = tree_ys\n",
    "    \n",
    "    return trees_dict\n",
    "\n",
    "tr = fit_trees(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This function plots the x and y data as well as the decision tree decisions for the predicted y:\n",
    "\n",
    "def tree_plotter(d, tr, label, treenum):\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "        \n",
    "    x = d['x']\n",
    "    y_true = d[label]\n",
    "    y_pred = tr[label][treenum]\n",
    "    \n",
    "    ax = sns.regplot(x=x, y=y_true, scatter_kws={'s':70}, fit_reg=False, x_ci=None, ci=None)\n",
    "    \n",
    "    ax.plot(x, y_pred, c=\"#D7B734\", linewidth=5)\n",
    "    \n",
    "    ax.set_title('Tree depth: '+str(treenum)+'\\n', fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Widget inputs code:\n",
    "\n",
    "def tree_plot_vars(function='ysq', treenum=1):\n",
    "    tree_plotter(d, tr, function, treenum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "widgets.interact(tree_plot_vars,\n",
    "                 function=('ysq','yexp','ysin','ygenlog'),\n",
    "                 treenum=widgets.IntSlider(min=1, max=10, step=1, continuous_update=False, value=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we increase tree depth from 1 to 10, what do we notice? Does this suggest that we need to be wary of anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make a single regression tree model with depth 3 for x**2 function:\n",
    "# \n",
    "x = d['x']\n",
    "y = d['ysq']\n",
    "\n",
    "dtree = DecisionTreeRegressor(max_depth=3)\n",
    "dtree.fit(x[:, np.newaxis], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# pip install pydot2\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()  \n",
    "\n",
    "export_graphviz(dtree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decisions for y based on the decision tree DAG above\n",
    "\n",
    "tree_plotter(d, tr, 'ysq', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Addressing overfitting\n",
    "\n",
    "A stopping criterion determines when to no longer construct further nodes. \n",
    "\n",
    "We can stop when all records belong to the same class, or when all records have the same attributes. This [**maximizes variance at the expense of bias**](http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/), leading to overfitting. \n",
    "\n",
    "**Setting a maximum depth:**\n",
    "\n",
    "A simple way to prevent overfitting is to set a hard limit on the \"depth\" of the decision tree.\n",
    "\n",
    "**Minimum observations to make a split:**\n",
    "\n",
    "An alternative to maximum depth (and can be used at the same time), is to specify the minimum number of datapoints reqired to make a split at a node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CART advantages\n",
    "\n",
    "- Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\n",
    "    - Useful to work with non technical departments (marketing/sales).\n",
    "- Requires little data preparation. \n",
    "    - Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.\n",
    "- Able to handle both numerical and categorical data. \n",
    "    - Other techniques are usually specialized in analyzing datasets that have only one type of variable.\n",
    "- Uses a **white box** model.\n",
    "    - If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic.\n",
    "    - By contrast, in a **black box** model, the explanation for the results is typically difficult to understand, for example in neural networks.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "- Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.\n",
    "- Once trained can be implemented on hardware and has extremely fast execution.\n",
    "    - Real-time applications like trading, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CART disadvantages\n",
    "\n",
    "- Locally-optimal.\n",
    "    - Practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. \n",
    "    - Such algorithms cannot guarantee to return the globally-optimal decision tree.\n",
    "- Overfitting.\n",
    "    - Decision-tree learners can create over-complex trees that do not generalize well from the training data.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large.\n",
    "    - Neural networks, for example, are superior for these problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "bf466222cfe14de2a29afb536046dfdd": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

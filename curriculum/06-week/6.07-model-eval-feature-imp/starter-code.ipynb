{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Model Evaluation & Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Explain how feature importance is calculated for decision trees\n",
    "- Calculate feature importance manually\n",
    "- Extract feature importance with scikit-learn\n",
    "- Extend the calculation to ensemble models (RF, ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add graphviz in the terminal; you will need this later.\n",
    "# $ pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening \n",
    "Today we will discuss feature importance for tree based models. When we build a machine learning model we are not only interested in pure prediction accuracy. More often than not we are building a model to get insights on what the relevant predictor variables are.\n",
    "\n",
    "E.g. you have 1000 features to predict user retention.\n",
    "Which features are relevant? Can you identify them? Can you build marketing strategies to address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for non-parametric models \n",
    "\n",
    "We previously discussed feature selection in the context of logistic regression. Logistic regression is a parametric model, which means that our hypothesis is described in terms of coefficients that we tune to improve the model's accuracy. In particular, since LR is a linear model, each parameter is associated to a specific feature. Thus, if the features are normalized, we can interpret the size of each coefficient to indicate the relative importance of that specific feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Tree based models are non-parametric, thus we don't have coefficients to tune like we did in linear models. We can however still ask which of the features are more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a tree, we can compute how much each feature decreases the weighted impurity by adding up all the impurity gains where such a feature is used to determine a split. In other words, the importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Feature importance in Decision Trees\n",
    "\n",
    "Suppose you are working at a car company and you are tasked to identify which features drive the acceptability of a car. You have collected some data on several features including:\n",
    "\n",
    "    - PRICE                  overall price\n",
    "        - buying             buying price\n",
    "        - maint              price of the maintenance\n",
    "    - TECH                   technical characteristics\n",
    "        - COMFORT            comfort\n",
    "            - doors          number of doors\n",
    "            - persons        capacity in terms of persons to carry\n",
    "            - lug_boot       the size of the trunk\n",
    "        - safety             estimated safety of the car\n",
    "\n",
    "(This is the car dataset we used previously.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature values are:\n",
    "\n",
    "    buying       v-high, high, med, low\n",
    "    maint        v-high, high, med, low\n",
    "    doors        2, 3, 4, 5-more\n",
    "    persons      2, 4, more\n",
    "    lug_boot     small, med, big\n",
    "    safety       low, med, high\n",
    "\n",
    "Class Distribution (number of instances per class):\n",
    "\n",
    "    class      N          N[%]\n",
    "    -----------------------------\n",
    "    unacc     1210     (70.023 %) \n",
    "    acc        384     (22.222 %) \n",
    "    good        69     ( 3.993 %) \n",
    "    v-good      65     ( 3.762 %) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all let's load it and map it to binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "df = pd.read_csv('../6.05-random-forests/assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will encode the features using a One Hot encoding scheme, i.e. we will consider them as categorical variables.\n",
    "\n",
    "Since Scikit-Learn doesn't understand strings, but only numbers we will also need to map the labels to numbers. We can use the `LabelEncoder` we've encountered other times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a decision tree on the whole dataset (ignore overfitting for the moment). Let's also artificially constrain the tree to be small so that we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2)\n",
    "\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "export_graphviz(dt, out_file=\"mytree.dot\")\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['persons']=='2'].acceptability.value_counts()\n",
    "print X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first choice involves `person_2`. If the car only takes 2 people (`person_2` == 1) then the condition (our target class) is unacceptable. This happens in 33% of the cases. Note that the leaf under the `False` branch is 100% pure, and therefore its Gini measure is 0.0.\n",
    "\n",
    "On the other hand, if the car can hold more than 2 people, we will need to consider other choices. For example if the car is unsafe, then it's also unacceptable. And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the target is a classification taking values $1, \\ldots, K-1, K$. If node $m$ represents a region $R_m$ with $N_m$ observations, the proportion of class $k$ observations in node $m$ can be written as:\n",
    "$$\n",
    "        C_k = \\frac{1}{N_m} \\sum_{x_i\\in R_m} I(y_i = k)\n",
    "$$\n",
    "The Gini Index is then defined as:\n",
    "$$\n",
    "        \\text{Gini}= \\sum_{k=1}^{K} C_k (1 - C_k)\n",
    "              = 1 - \\sum_{k=1}^{K} C_k^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C0 = np.linspace(0,1)\n",
    "C1 = 1.0 - C0\n",
    "\n",
    "gini = 1 - C0**2 - C1**2\n",
    "\n",
    "plt.plot(C0, gini)\n",
    "plt.title('Gini index for a Binary Classification')\n",
    "plt.xlabel('Fraction of samples in class 0')\n",
    "plt.ylabel('Gini index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the calculation of the Gini index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_node_values = [0.22, 0.04, 0.7, 0.04] ## Proportion of results from root node.\n",
    "\n",
    "def gini(values):\n",
    "    tot = 0.0\n",
    "    for val in values:\n",
    "        tot += val ** 2\n",
    "    \n",
    "    return 1.0 - tot\n",
    "\n",
    "gini(root_node_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are ready to look at feature importances in our tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(dt.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance will be \n",
    "\n",
    "$$Importance = Gini(parent) - \\sum_k C_k\\cdot Gini(child_k)$$\n",
    "\n",
    "where $C_k$ is the proportion of parent observations in class $k$.\n",
    "\n",
    "**Note:** This is just the purity gain (information gain) from yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** How could we go through and calculate this ourselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Feature importance in Ensemble models\n",
    "\n",
    "Scikit Learn implements feature importance for random forest and extra trees methods.\n",
    "\n",
    "Let's train one of each of these and investigate the feature importance:\n",
    "\n",
    "- Random Forest\n",
    "- Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "et = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rf.fit(X, y)\n",
    "et.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest exposes the feature importance and it calculates it as the average feature importance of the trees. Let's verify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all(rf.feature_importances_ == np.mean([tree.feature_importances_ for tree in rf.estimators_], axis=0))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "# calculate the standard deviation of feature importances by looping over the trees in the random forest\n",
    "# \n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all(rf.feature_importances_ == np.mean([tree.feature_importances_ for tree in rf.estimators_], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn: repeat the investigation for the extra trees model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all(rf.feature_importances_ == np.mean([tree.feature_importances_ for tree in rf.estimators_], axis=0))\n",
    "\n",
    "importances = et.feature_importances_\n",
    "# calculate the standard deviation of feature importances by looping over the trees in the random forest\n",
    "# \n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in et.estimators_], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compare the 3 models (re-init Decision Tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "\n",
    "importances = pd.DataFrame(zip(dt.feature_importances_,\n",
    "                               rf.feature_importances_,\n",
    "                               et.feature_importances_),\n",
    "                           index=X.columns,\n",
    "                           columns=['dt_importance',\n",
    "                                    'rf_importance',\n",
    "                                    'et_importance']).sort_values('rf_importance',\n",
    "                                                                   ascending=False)\n",
    "\n",
    "                           \n",
    "importances.plot(kind='bar')\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Gini Importance](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)\n",
    "- [DT Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [Feature Importance in Sklearn Blog](http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/)\n",
    "- [Plot Feature Importances example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "- [Selecting good features](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

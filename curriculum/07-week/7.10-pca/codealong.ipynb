{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis Recap\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Recall from our linear algebra lecture on principal component analysis that PCA is a method we can use to ensure that our features in a linear model are independent of one another. PCA is an example of **dimensionality reduction**.\n",
    "#### Dimensionality reduction has a number of advantages:\n",
    "- Increases computational efficiency when fitting models.\n",
    "- Can help with addressing a multicollinearity problem.\n",
    "- Makes visualization simpler (or feasible).\n",
    "\n",
    "#### Check: What are some potential pitfalls to dimensionality reduction?\n",
    "Dimensionality reduction can generally be broken down into one of two categories:\n",
    "- Feature Elimination\n",
    "- Feature Extraction\n",
    "\n",
    "As we've discussed, dimensionality reduction can be used as an exploratory/unsupervised learning method or as a pre-processing step for supervised learning later.\n",
    "\n",
    "## Principal Component Analysis\n",
    "Principal component analysis (PCA) is a method of finding **linear combinations** of your predictors (called \"principal components\") that explain, in order, the maximum possible variance in your predictors while ensuring that each principal component is orthogonal (independent) to all other principal components.\n",
    "\n",
    "Intuitively, PCA takes your \"cloud\" of data and rotates the axes so that the axes become the most concise, informative descriptors of your data as a whole. These axes are your \"principal components.\n",
    "\n",
    "### Proportion of Explained Variance\n",
    "Consider the total variance in your predictors $X_1$, $X_2$, $\\ldots$, $X_p$ to be $Var(X_1)+Var(X_2)+\\cdots+Var(X_p)$.\n",
    "\n",
    "If you find the your eigenvectors $\\lambda_1$, $\\lambda_2$, $\\ldots$, $\\lambda_p$, you'll notice that the sum of the eigenvectors equals the sum of the variances of the features.\n",
    "- That does **not** mean that $\\lambda_1=Var(X_1$), $\\lambda_2=Var(X_2)$, etc.\n",
    "\n",
    "Without loss of generality, let's assume that the eigenvalues are sorted from largest to smallest and their eigenvectors are sorted accordingly. Then, the fraction of explained variance in our model is given by:\n",
    "\n",
    "$$\\text{explained_variance} = \\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{i=1}^p\\lambda_i}$$\n",
    "\n",
    "where $\\lambda_1,\\ldots,\\lambda_k$ are the eigenvalues corresponding to the eigenvectors chosen for PCA and eigenvectors corresponding to $\\lambda_{k+1},\\ldots,\\lambda_p$ are the $p-k$ excluded eigenvectors.\n",
    "\n",
    "### Assumptions\n",
    "1. PCA assumes linearity between your variables.\n",
    "2. PCA assumes that components with larger variance correspond to more important dynamics (signal) and that less variances corresponds with less important dynamics (noise).\n",
    "3. PCA assumes that the mean and standard deviation are important.\n",
    "4. *Some PCA algorithms assume Normality when working with missing data.\n",
    "\n",
    "### Use Cases\n",
    "1. Image Processing\n",
    "2. 20+ Variables with High Multicollinearity\n",
    "3. 100+ Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Linear Regression Codealong\n",
    "In this detailed codealong/lab we are going to practice and look more extensively at PCA.\n",
    "\n",
    "PCA is one of the more difficult concepts/algorithms in this class to understand well in such a short amount of time, but considering how often people use it to simplify their data, reduce noise in their data, and find unmeasured \"[latent variables](https://en.wikipedia.org/wiki/Latent_variable),\" it is important to spend the time to understand what's going on.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset we are using for this lab is a subset of the [much more detailed speed dating dataset](https://www.kaggle.com/annavictoria/speed-dating-experiment). In particular, this contains no information on the actual speed dating itself (successes with or opinions of other individuals). It also contains no \"follow-up\" information where individuals are re-asked the same questions about themselves. All it contains are things that an individual enjoys doing, their ratings of themselves on how desireable they are, and how they think others rate them on desireability.\n",
    "\n",
    "Specifically, the columns in the data are outlined below:\n",
    "```\n",
    "        subject_id                   :   unique individual identifier\n",
    "        like_sports                  :   enjoyment of participating in sports\n",
    "        like_tvsports                :   enjoyment of watching sports on tv\n",
    "        like_exercise                :   enjoyment of exercise\n",
    "        like_food                    :   enjoyment of food\n",
    "        like_museums                 :   enjoyment of museums\n",
    "        like_art                     :   enjoyment of art\n",
    "        like_hiking                  :   enjoyment of hiking\n",
    "        like_gaming                  :   enjoyment of playing games\n",
    "        like_clubbing                :   enjoyment of going clubbing/partying\n",
    "        like_reading                 :   enjoyment of reading\n",
    "        like_tv                      :   enjoyment of TV in general\n",
    "        like_theater                 :   enjoyment of the theater (plays, musicals, etc.)\n",
    "        like_movies                  :   enjoyment of movies\n",
    "        like_concerts                :   enjoyment of concerts\n",
    "        like_music                   :   enjoyment of music\n",
    "        like_shopping                :   enjoyment of shopping\n",
    "        like_yoga                    :   enjoyment of yoga\n",
    "        subjective_attractiveness    :   how attractive they rate themselves\n",
    "        subjective_sincerity         :   how sincere they rate themselves\n",
    "        subjective_intelligence      :   how intelligent they rate themselves\n",
    "        subjective_fun               :   how fun they rate themselves\n",
    "        subjective_ambition          :   how ambitious they rate themselves\n",
    "        objective_attractiveness     :   percieved rating others would give them on how attractive they are\n",
    "        objective_sincerity          :   percieved rating others would give them on how sincere they are\n",
    "        objective_intelligence       :   percieved rating others would give them on how intelligent they are\n",
    "        objective_fun                :   percieved rating others would give them on how fun they are\n",
    "        objective_ambition           :   percieved rating others would give them on how ambitious they are\n",
    "```    \n",
    "\n",
    "There are 551 subjects total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = pd.read_csv('speed_dating_user_attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease, I'm going to be dropping the objective columns, since they are about half null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd.drop(['objective_attractiveness','objective_sincerity',\n",
    "         'objective_intelligence','objective_fun','objective_ambition'],\n",
    "         axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd.dropna(inplace=True) ## Drop the NAs from the other columns as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd.isnull().sum() ## Double check to make sure that we don't have any more nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## List comprehension to select the subjective columns\n",
    "subjective_cols = [col for col in sd.columns if col.startswith('subjective')]\n",
    "print subjective_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjective = sd[subjective_cols]\n",
    "\n",
    "## Remember, we need to center before PCA and some like to normalize.\n",
    "subjective = (subjective - subjective.mean()) / subjective.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## If our features are independent of one another, PCA won't do much of anything.\n",
    "## We can easily check for multicollinearity with pairplot.\n",
    "\n",
    "sns.pairplot(subjective, kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We can also take a look at the \"like\" columns.\n",
    "\n",
    "sd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Repeat the same process as above: Select the columns, normalize the features, and then check the pairplot.\n",
    "like_cols = ['like_tvsports','like_sports','like_museums','like_theater','like_shopping']\n",
    "sd_like = sd[like_cols]\n",
    "sd_like = (sd_like - sd_like.mean()) / sd_like.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(sd_like, kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do PCA on the subjective ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize the PCA from SKLearn. We're using 5 components because we have five input features.\n",
    "subjective_pca = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Fit the PCA\n",
    "subjective_pca.fit(subjective.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New term: Loadings\n",
    "\n",
    "Principal component loadings are ways to combine the eigenvectors (direction) and eigenvalues (magnitude) of our different principal components.\n",
    "\n",
    "$$ \\text{principal component loading i} = \\text{eigenvector i} * \\sqrt{\\text{eigenvalue i}} $$\n",
    "\n",
    "[Read more in this StackExchange Q&A.](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Return the loadings for each of the components\n",
    "subj_components = subjective_pca.components_\n",
    "subj_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Return the loadings for each PCA, along with the column names that corresponding to each loading.\n",
    "\n",
    "print subjective_cols\n",
    "print '-------------------------------------'\n",
    "\n",
    "for i, pc in enumerate(['PC1','PC2','PC3','PC4','PC5']):\n",
    "    print pc, 'weighting vector:', subj_components[i]\n",
    "    print '-------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Return the eigenvalues and the explained variance.\n",
    "## NOTE: In sklearn, explained_variance_ returns the eigenvalues\n",
    "## while explained_variance_ratio_ gives us the actual explained variance.\n",
    "\n",
    "subj_exp_var_eigenvals = subjective_pca.explained_variance_\n",
    "subj_exp_var_pct = subjective_pca.explained_variance_ratio_\n",
    "\n",
    "print 'eigenvalues:', subj_exp_var_eigenvals\n",
    "print ''\n",
    "print 'explained variance pct:', subj_exp_var_pct\n",
    "print ''\n",
    "print subj_exp_var_pct.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Call transform on the values. This will actually apply the PCA transformation to the data.\n",
    "\n",
    "subj_to_pcs = subjective_pca.transform(subjective.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## If our features are independent of one another, PCA won't do much of anything.\n",
    "## We can easily check for multicollinearity with pairplot.\n",
    "\n",
    "sns.pairplot(subjective, kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(subj_to_pcs), kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Here, we're taking a look at the normalized values for each of the subjective features\n",
    "## for the first observation, which we'll call person one.\n",
    "\n",
    "person1_original_ratings = subjective.iloc[0, :]\n",
    "print person1_original_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Here, we're selecting from the sklearn output the transformed data for observation one, \n",
    "## for each of the principal components. Recall that we returned this from subj_to_pcs above.\n",
    "\n",
    "person1_pcas = subj_to_pcs[0, :]\n",
    "print person1_pcas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with the \"like\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "reading = sd['like_reading'].values\n",
    "reading[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_lr = LinearRegression()\n",
    "\n",
    "## Here, we are feeding a linear regression, as well as the normalized \"subjective\" features.\n",
    "\n",
    "original_scores = cross_val_score(original_lr, subjective.values, reading, cv=10)\n",
    "\n",
    "print original_scores\n",
    "print ''\n",
    "print np.mean(original_scores)\n",
    "print ''\n",
    "print np.std(original_scores)\n",
    "print ''\n",
    "print(np.mean(original_scores) - 1.96*np.std(original_scores),np.mean(original_scores) + 1.96*np.std(original_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_lr = LinearRegression()\n",
    "\n",
    "## Repeat the same process, however with the \"subjective\" features transformed by the PCA\n",
    "\n",
    "pca_scores = cross_val_score(pca_lr, subj_to_pcs, reading, cv=10)\n",
    "\n",
    "print pca_scores\n",
    "print ''\n",
    "print np.mean(pca_scores)\n",
    "print ''\n",
    "print np.std(pca_scores)\n",
    "print ''\n",
    "print(np.mean(pca_scores) - 1.96*np.std(pca_scores),np.mean(pca_scores) + 1.96*np.std(pca_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print subjective.values[0]\n",
    "\n",
    "print subj_to_pcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print original_lr.fit(subjective.values, reading).score(subjective.values, reading)\n",
    "\n",
    "print pca_lr.fit(subj_to_pcs, reading).score(subj_to_pcs, reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: [How many components should we keep?](https://medium.com/@matthew.w.brems/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "lr = LinearRegression()\n",
    "\n",
    "# make a pipeline that chains together the pca and the linear regression\n",
    "# this means that when the X data gets \"piped in\" it first hits the PCA,\n",
    "# which will fit it to the data, then transform the original variables\n",
    "# into their principal component \"new variables\".\n",
    "\n",
    "# From here these principal components get sent into the linear regression\n",
    "# to be fit.\n",
    "\n",
    "# This is very useful because we can actually gridsearch the number of \n",
    "# components - gridsearch is designed to run on models that return\n",
    "# some kind of score. The PCA has no score, it's just transforming the\n",
    "# variables. So gridsearching the PCA itself is meaningless.\n",
    "\n",
    "# However, if it then goes to a linear regression after, we can score\n",
    "# it on some dependent variable in order to determine which number\n",
    "# of components was the best one!\n",
    "\n",
    "pca_pipe = make_pipeline(pca, lr)\n",
    "\n",
    "pca_grid = {\n",
    "    'pca__n_components':[1,2,3,4,5]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Gridsearch now takes the pipeline:\n",
    "pca_gs = GridSearchCV(pca_pipe, pca_grid, cv=10)\n",
    "\n",
    "# fit it on the data X, y as usual\n",
    "pca_gs.fit(subjective.values, reading)\n",
    "\n",
    "print pca_gs.best_params_\n",
    "print pca_gs.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

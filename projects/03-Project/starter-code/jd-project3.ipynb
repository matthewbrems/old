{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary classifier.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use a random forest classifier, as well as another classifier of your choice; either logistic regression, SVM, or KNN. \n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com. In the second, we'll focus on using listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\")\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "The URL here has many query parameters\n",
    "- q for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- l for a location\n",
    "- start for what result number to start on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "url = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&fromage=last&start=\"\n",
    "x=0\n",
    "url_start = url+str(x) # if we set the start variable in the URL to 0 to begin with, it will first pull up results 1-10\n",
    "\n",
    "page = requests.get(url_start).content\n",
    "soup = BeautifulSoup(page,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write 4 functions to extract each item: location, company, job, and salary.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_location(webpage):\n",
    "    tag = webpage.find('span', attrs={'class':'location'})\n",
    "    return tag.text\n",
    "    \n",
    "def get_company(webpage):\n",
    "    tag = webpage.find('span', attrs={'class':'company'})\n",
    "    return tag.text.strip('\\n')\n",
    "\n",
    "def get_job(webpage):\n",
    "        tag = webpage.find('a', title=True, attrs={'data-tn-element':'jobTitle'})\n",
    "        try:\n",
    "            return tag['title']\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "def get_salary(webpage):\n",
    "    try:\n",
    "        return webpage.find('table').tr.td.nobr.renderContents()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_description(webpage):\n",
    "    description = webpage.find('span', attrs={'itemprop':\"description\"})\n",
    "    try:\n",
    "        return description.text.strip('\\n')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete the following code to collect results from multiple cities and starting points.\n",
    "- Enter your city below to add it to the search\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR_CITY = ''\n",
    "\n",
    "#### I am not scraping for a particular list of cities, just all cities in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "6e259594-1c52-436b-ab9e-527e071941c1"
   },
   "outputs": [],
   "source": [
    "### I chose to write a function to combine some of the steps\n",
    "### I also chose not to clean the salaries each time, but instead\n",
    "### will just clean them all at once, when I import the data from the csvs\n",
    "\n",
    "\n",
    "## define two functions that will be used in the main scraping function\n",
    "\n",
    "def numbers_commas_to_int(string):\n",
    "    import locale # given a string of a number with commas, convert to float\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') #for american comma notation\n",
    "    # if european comma notation needed, change 2nd parameter to 'fr_FR'\n",
    "    num = locale.atof(string)\n",
    "    return float(num)\n",
    "\n",
    "\n",
    "## import the results that have been previously exported\n",
    "def compile_files():\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    indeed_csvs = '/Users/jennydoyle/Desktop/dsi/indeed/'\n",
    "    files = glob.glob(indeed_csvs + '*.csv')\n",
    "    indeed_final = pd.DataFrame(columns=['job','company','location','salary','description'])\n",
    "    for f in files:\n",
    "        f = pd.read_csv(f, names=['job','company','location','salary','description'],low_memory=False)\n",
    "        indeed_final = indeed_final.append(f)\n",
    "    indeed_final.drop_duplicates(inplace=True)\n",
    "    return indeed_final\n",
    "\n",
    "\n",
    "######################################################\n",
    "######################################################\n",
    "######################################################\n",
    "\n",
    "def scrape_indeed():\n",
    "    ## compile previously scraped results to see if there are new jobs to add\n",
    "    indeed = compile_files()    \n",
    "    base = len(indeed)\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import datetime\n",
    "    import time\n",
    "    import re\n",
    "    import numpy as np\n",
    "    start = datetime.datetime.now()\n",
    "    print 'Start time: ',start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print 'Base file has ', base, ' records'\n",
    "    \n",
    "    # save content of URL to variable page\n",
    "    url = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&fromage=last&start=\"\n",
    "    x=0\n",
    "    url_start = url+str(x) # if we set the start variable in the URL to 0 to begin with, it will first pull up results 1-10\n",
    "\n",
    "    page = requests.get(url_start).content\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    print 'Page scraped & souped'\n",
    "\n",
    "    for results in soup.find('div', attrs={'id':'searchCount'}):\n",
    "        count = str(results).split()    # take the full line that says 'Jobs x to y of z' and turn into a list\n",
    "        total = count[len(count)-1]          # set total to z, the total number o[f results\n",
    "        total = numbers_commas_to_int(total) # since there are commas if the number > 999, this function will deal with that and convert to int\n",
    "    \n",
    "    x = 0 # if we set the start variable in the URL to 0 to begin with, it will first pull up results 1-10\n",
    "\n",
    "    while x <= total:\n",
    "        url_new_page = url + str(x)\n",
    "        page = requests.get(url_new_page).content\n",
    "        soup = BeautifulSoup(page)\n",
    "        \n",
    "        for num_listings in  soup.find('div', attrs={'id':'searchCount'}) :\n",
    "            num_listings = num_listings.split()[3]\n",
    "        \n",
    "        main = soup.find('td',{'id':'resultsCol'})   # limit our searching to solely the results portion of the page\n",
    "        results = main.find_all('div', {'class': re.compile(\"result$\")}) # create a list consisting only of the 15 results\n",
    "\n",
    "        for i in range(len(results)):\n",
    "            job = get_job(results[i])\n",
    "            company = get_company(results[i])         # put all companies for each posting on curent results page into companies list\n",
    "            location = get_location(results[i])       # put all locations for each posting on current results page into locations list\n",
    "            salary = get_salary(results[i])           # put all salaries for each posting on current results page into salaries list\n",
    "            description = get_description(results[i]) # put all descriptions for each posting on current results page into descriptions list\n",
    "\n",
    "            add_job = [job, company, location, salary, description]\n",
    "            indeed.append(add_job)\n",
    "        x+=10\n",
    "        new = len(indeed) - base\n",
    "        elapsed = datetime.datetime.now() - start\n",
    "        remaining = total - x\n",
    "        est_pages = remaining/10\n",
    "        \n",
    "        \n",
    "        print 'Added ', new, ' jobs-- scraped ',num_listings,' of ', total, ' listings in ', elapsed, '; ', est_pages, ' pages remaining'\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "            \n",
    "    finish = datetime.datetime.now()\n",
    "    now = finish.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print 'Finish time: ',now\n",
    "\n",
    "    elapsed = finish-start\n",
    "    print 'Elapsed: ',elapsed\n",
    "    indeed = pd.DataFrame(indeed)\n",
    "    indeed.to_csv('/Users/jennydoyle/Desktop/dsi/indeed/'+now+'.csv',sep=',', encoding='utf-8',header=False,index=False)\n",
    "    return indeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now\n",
    "1. Some of the entries may be duplicated\n",
    "1. The salaries are given as text and usually with ranges.\n",
    "\n",
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "58533e57-f86b-494a-b841-e7b59c6229c6"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "## I've decided to clean data after re-importing all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "7d4bc860-b214-4f75-9cd0-b234830b1ec2"
   },
   "source": [
    "#### Write a function that takes a salary string and converts it to a number, averaging a salary range if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "a0f701e0-80bd-40ba-9101-4535860c0968"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "#numbers_commas_to_int defined with main scrape_indeed function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "\n",
    "## in main function scrape_indeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting salaries using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "#### Load in the the data of scraped salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Novetta</td>\n",
       "      <td>Crystal City, VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Syntelli Solutions, Inc</td>\n",
       "      <td>Charlotte, NC 28277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>Software Engineer (Data and Analytics)</td>\n",
       "      <td>The Advisory Board Company</td>\n",
       "      <td>Richmond, VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>TechStratium Inc.</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TechStratium is hiring Data Scientists to join...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>Advanced Analytics Data Scientist</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Springfield, VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As an Advanced Analytics Data Scientist, you'l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        job  \\\n",
       "0.0                          Data Scientist   \n",
       "1.0                          Data Scientist   \n",
       "2.0  Software Engineer (Data and Analytics)   \n",
       "3.0                          Data Scientist   \n",
       "4.0       Advanced Analytics Data Scientist   \n",
       "\n",
       "                                company             location salary  \\\n",
       "0.0                             Novetta     Crystal City, VA    NaN   \n",
       "1.0             Syntelli Solutions, Inc  Charlotte, NC 28277    NaN   \n",
       "2.0          The Advisory Board Company         Richmond, VA    NaN   \n",
       "3.0                   TechStratium Inc.           McLean, VA    NaN   \n",
       "4.0                                 IBM      Springfield, VA    NaN   \n",
       "\n",
       "                                           description  \n",
       "0.0                                                NaN  \n",
       "1.0                                                NaN  \n",
       "2.0                                                NaN  \n",
       "3.0  TechStratium is hiring Data Scientists to join...  \n",
       "4.0  As an Advanced Analytics Data Scientist, you'l...  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "indeed = compile_files()\n",
    "\n",
    "indeed.head()\n",
    "# indeed.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=indeed[indeed.salary.notnull()]\n",
    "df.salary = df.salary.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['salary_list'] = df.salary.str.split()\n",
    "\n",
    "mask = df.salary.str.contains('-')\n",
    "\n",
    "df['low_end'], df['high_end'], df['salary_clean'], df['period'] = np.NaN, np.NaN, np.NaN, np.NaN\n",
    "df['low_end'][mask] = map(lambda x: x[0],df.salary_list.loc[mask])\n",
    "df['high_end'][mask] = map(lambda x: x[2],df.salary_list.loc[mask])\n",
    "\n",
    "mask = df.salary.str.contains('year')\n",
    "df['period'] = ''\n",
    "df['period'][mask] = map(lambda x: 1,df.salary_list.loc[mask])\n",
    "\n",
    "mask = df.salary.str.contains('month')\n",
    "df['period'][mask] = map(lambda x: 12,df.salary_list.loc[mask])\n",
    "\n",
    "mask = df.salary.str.contains('hour')\n",
    "df['period'][mask] = map(lambda x: 2080,df.salary_list.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df.low_end[df.high_end==1] = np.NaN\n",
    "df.high_end[df.high_end==1] = np.NaN\n",
    "\n",
    "df['salary_clean'][df.salary.notnull()]= [x[0].strip('$') for x in df.salary_list[df.salary_list.notnull()]]\n",
    "df['low_end'][df.low_end.notnull()]= df.low_end.str.strip('$')\n",
    "df['high_end'][df.high_end.notnull()]= df.high_end.str.strip('$')\n",
    "\n",
    "df.salary_clean[df.low_end.notnull()&df.high_end.notnull()] = np.NaN\n",
    "df.salary_clean[df.salary_clean=='salary'] = np.NaN\n",
    "\n",
    "df.salary_clean[df.salary_clean.notnull()] = [numbers_commas_to_int(x) for x in df.salary_clean[df.salary_clean.notnull()]]\n",
    "df.low_end[df.low_end.notnull()] = [numbers_commas_to_int(x) for x in df.low_end[df.low_end.notnull()]]\n",
    "df.high_end[df.high_end.notnull()] = [numbers_commas_to_int(x) for x in df.high_end[df.high_end.notnull()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.salary_clean[df.salary_clean.isnull()] = (df.low_end + df.high_end) / 2\n",
    "df.salary_clean[df.salary_clean.notnull()&df.period.notnull()] = df.period * df.salary_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't _have_ to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "df = df[df.salary_clean.notnull()]\n",
    "median_salary = np.median(df.salary_clean)\n",
    "df['high_salary'] = True\n",
    "df['high_salary'][df.salary_clean <= median_salary] = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49295775,  0.5       ,  0.5       ])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "dummies = pd.get_dummies(df[['job','company','location']])\n",
    "\n",
    "X = pd.concat([dummies, df['salary_clean']], axis=1)\n",
    "y = df.high_salary\n",
    "\n",
    "cross_val_score(LogisticRegression(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low salary using Sklearn. Start by ONLY using the location as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## clean up locations\n",
    "# remove areas in parentheses\n",
    "df.location = df.location.str.replace('\\((.*?)\\)','')\n",
    "df.location = df.location.str.strip()\n",
    "# remove zip codes\n",
    "df.location = df.location.str.replace(r'(\\d{5}(\\-\\d{4})?)$','')\n",
    "df.location = df.location.str.strip()\n",
    "# create feature with states\n",
    "df['state'] = df.location.str.findall('\\,\\s(\\D{2})$')\n",
    "# remove state from location\n",
    "df.location = df.location.str.replace('(\\,\\s\\D{2})$','')\n",
    "\n",
    "# take the states out of the list they were for some reason placed in\n",
    "df.state = df.state.astype(str)\n",
    "df.state = df.state.str.replace('(\\[)','')\n",
    "df.state = df.state.str.replace('(\\])','')\n",
    "df.state = df.state.str.replace('(\\')','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# get dummies!!\n",
    "X = pd.get_dummies(df[['location','state']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> Score:\t0.645 ± 0.035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "def classify(Classifier, X, y, weight):\n",
    "    name = str(Classifier)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=41)\n",
    "    if weight != '':\n",
    "        dt = Classifier(class_weight=weight)\n",
    "    else:\n",
    "        dt = Classifier()\n",
    "    s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "    print \"{} Score:\\t{:0.3} ± {:0.3}\".format(name, s.mean().round(3), s.std().round(3))\n",
    "\n",
    "\n",
    "classify(RandomForestClassifier, X, y, 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a job title.\n",
    "- For example, create a feature that represents whether 'Senior' is in the title \n",
    "- or whether 'Manager' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "068dc1cf-7fd7-4f27-a1f1-7f0a5a221d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "df.job = df.job.str.upper()\n",
    "df['analyst'] = 0\n",
    "df['analyst'][df.job.str.contains('ANALY')] = 1\n",
    "\n",
    "df['statistician'] = 0\n",
    "df['statistician'][df.job.str.contains('STATISTIC')] = 2\n",
    "\n",
    "df['machine_learning'] = 0\n",
    "df['machine_learning'][df.job.str.contains('MACHINE')] = 2\n",
    "\n",
    "df['research'] = 0\n",
    "df['research'][df.job.str.contains('RESEARCH')] = 1\n",
    "\n",
    "df['engineer'] = 0\n",
    "df['engineer'][df.job.str.contains('ENGIN')] = 2\n",
    "\n",
    "df['mid_level'] = 0\n",
    "df['mid_level'][df.job.str.contains('MANAGER')] = 2\n",
    "df['mid_level'][df.job.str.contains('MID_LEVEL')] = 2\n",
    "df['mid_level'][df.job.str.contains('\\WII\\W')] = 2\n",
    "df['mid_level'][df.job.str.contains('\\WII$')] = 2\n",
    "df['mid_level'][df.job.str.contains('2')] = 2\n",
    "df['mid_level'][df.job.str.contains('ASSISTANT')] = 2\n",
    "\n",
    "df['entry_level'] = 0\n",
    "df['entry_level'][df.job.str.contains('\\WI\\W')] = 1\n",
    "df['entry_level'][df.job.str.contains('\\WI$')] = 1\n",
    "df['entry_level'][df.job.str.contains('ENTRY_LEVEL')] = 1\n",
    "df['entry_level'][df.job.str.contains('1')] = 1\n",
    "df['entry_level'][df.job.str.contains('PART-TIME')] = 1\n",
    "df['entry_level'][df.job.str.contains('INTERN')] = 1\n",
    "\n",
    "df['senior_level'] = 0\n",
    "df['senior_level'][df.job.str.contains('\\WIII\\W')] = 3\n",
    "df['senior_level'][df.job.str.contains('\\WIII$')] = 3\n",
    "df['senior_level'][df.job.str.contains('3')] = 3\n",
    "df['senior_level'][df.job.str.contains('SR\\W')] = 3\n",
    "df['senior_level'][df.job.str.contains('SENIOR')] = 3\n",
    "df['senior_level'][df.job.str.contains('LEAD')] = 3\n",
    "df['senior_level'][df.job.str.contains('PRINCIPAL')] = 3\n",
    "df['senior_level'][df.job.str.contains('DIRECTOR')] = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9c9274ef-c9f5-4d56-b286-ecc8709eff9f"
   },
   "source": [
    "#### Rebuild this model with the new variables\n",
    "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n",
    "- Remember to scale the feature variables as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "b76f65cd-cd3a-4e91-af55-12880be7b057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77142857142857146"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df.location)\n",
    "df_final = pd.concat([dummies, df[['high_salary','salary_clean','analyst','statistician','engineer','machine_learning','research','mid_level','entry_level','senior_level']]], axis=1)\n",
    "X = pd.concat([dummies, df[['analyst','statistician','engineer','machine_learning','research','mid_level','entry_level','senior_level']]], axis=1)\n",
    "features = X.columns\n",
    "y = list(df.high_salary.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier().fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81690141,  0.82857143,  0.72857143])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "fead9b5b-7316-405d-87fd-e144dff0cbeb"
   },
   "source": [
    "#### Continue to incorporate other text features from the title or summary that you believe will predict the salary and examine their coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.15873364358172534, 'research'),\n",
       " (0.10900826024688226, 'analyst'),\n",
       " (0.065341740529850187, 'statistician'),\n",
       " (0.036614210819368755, 'Reston'),\n",
       " (0.034678687062958208, 'New York'),\n",
       " (0.031305129733286134, 'engineer'),\n",
       " (0.029896077624091699, 'Los Angeles'),\n",
       " (0.025327156776843579, 'Md City'),\n",
       " (0.0238659465421961, 'entry_level'),\n",
       " (0.023835508037411135, 'senior_level'),\n",
       " (0.02317681929165059, 'Jacksonville'),\n",
       " (0.022903099087745685, 'Iowa City'),\n",
       " (0.020236994633370569, 'Groton'),\n",
       " (0.019679664481310481, 'Washington'),\n",
       " (0.019101052694672407, 'Boston'),\n",
       " (0.018935812666367222, 'Richmond'),\n",
       " (0.0184801016371611, 'Queens'),\n",
       " (0.017111144992004366, 'Reno'),\n",
       " (0.017013001621228634, 'mid_level'),\n",
       " (0.015614114385209821, 'Chantilly'),\n",
       " (0.01465731428719442, 'Austin'),\n",
       " (0.013900159555269681, 'machine_learning'),\n",
       " (0.012006565212418571, 'Columbus'),\n",
       " (0.0118011777464536, 'Wake County'),\n",
       " (0.011689665950791173, 'Olympia'),\n",
       " (0.011330058743268843, 'Chicago'),\n",
       " (0.010569019765853402, 'Charlottesville'),\n",
       " (0.0093268364382379145, 'Bridgewater'),\n",
       " (0.0093118078325872371, 'Somerville'),\n",
       " (0.0085377428078431683, 'Santa Clara'),\n",
       " (0.0076775994619221553, 'East Hanover'),\n",
       " (0.0076356836255703732, 'Brooklyn'),\n",
       " (0.0076293972017519168, 'Manhattan'),\n",
       " (0.007588058559301873, 'Milford'),\n",
       " (0.007365846848153881, 'Ohio'),\n",
       " (0.0073545730119807164, 'Tucson'),\n",
       " (0.0067324977263472623, 'Fort Myers'),\n",
       " (0.0067134518789945963, 'Raymond'),\n",
       " (0.0064434288857943695, 'Franklin County'),\n",
       " (0.0058282843091187701, 'Berkeley Heights'),\n",
       " (0.0053259430065053679, 'McLean'),\n",
       " (0.0051982850113674118, 'Irvine'),\n",
       " (0.0049123885622499265, 'Columbia'),\n",
       " (0.0049024202120857927, 'Atlanta'),\n",
       " (0.0047144066970287116, 'Frisco'),\n",
       " (0.0045813577054769784, 'Richland County'),\n",
       " (0.0045019214568039548, 'Alexandria'),\n",
       " (0.0042337991735313576, 'Silver Spring'),\n",
       " (0.0036896353580665605, 'Mountain View'),\n",
       " (0.0035130521984757953, 'Saint Paul'),\n",
       " (0.0032184821689901283, 'Nashville'),\n",
       " (0.0027498233544080326, 'Bellevue'),\n",
       " (0.0026345010095309143, 'Madison'),\n",
       " (0.0026143552452656208, 'Houston'),\n",
       " (0.0024241134675226607, 'Boca Raton'),\n",
       " (0.0024054260299589008, 'Pleasanton'),\n",
       " (0.0019825247153772998, 'Sunnyvale'),\n",
       " (0.0019342179106880263, 'Miami'),\n",
       " (0.001876104705199852, 'Lacey'),\n",
       " (0.0016010219288907809, 'Oceanside'),\n",
       " (0.0015671201994297158, 'New Jersey'),\n",
       " (0.001467029766591268, 'Helena'),\n",
       " (0.001429195313363024, 'Palo Alto'),\n",
       " (0.0014216529308121091, 'Baltimore'),\n",
       " (0.001279298636220175, 'Denver'),\n",
       " (0.0011879772503584453, 'Albuquerque'),\n",
       " (0.00097022444878738334, 'Honolulu'),\n",
       " (0.00094793141863407787, 'Springfield'),\n",
       " (0.00087063715450410391, 'Philadelphia'),\n",
       " (0.00070613854852100122, 'Stony Brook'),\n",
       " (0.00069045066184479534, 'Menlo Park'),\n",
       " (0.00063859047529309608, 'Robbinsville'),\n",
       " (0.00044641773007711091, 'San Jose'),\n",
       " (0.00039655137619024029, 'Oakland'),\n",
       " (0.00032773374212672924, 'Las Vegas'),\n",
       " (0.00032745842953562753, 'Dearborn'),\n",
       " (0.00032725781849786775, 'Canton'),\n",
       " (0.00029960408037915529, 'Colorado Springs'),\n",
       " (0.00023954461040553849, 'Creve Coeur'),\n",
       " (0.0001296607612397142, 'Tumwater'),\n",
       " (0.00010851370851371189, 'San Antonio'),\n",
       " (0.0001047317293966079, 'Jefferson City'),\n",
       " (7.9754288914152472e-05, 'Chesterfield County'),\n",
       " (6.5408708752069046e-05, 'Durham'),\n",
       " (0.0, 'Albany'),\n",
       " (0.0, 'Anchorage'),\n",
       " (0.0, 'Arlington'),\n",
       " (0.0, 'Bremerton'),\n",
       " (0.0, 'Campbell'),\n",
       " (0.0, 'Charlotte'),\n",
       " (0.0, 'Coral Gables'),\n",
       " (0.0, 'Exeter'),\n",
       " (0.0, 'Jupiter'),\n",
       " (0.0, 'Kitsap County'),\n",
       " (0.0, 'Lawrence'),\n",
       " (0.0, 'Los Alamos'),\n",
       " (0.0, 'Los Angeles County'),\n",
       " (0.0, 'Nashville-Davidson'),\n",
       " (0.0, 'Newton'),\n",
       " (0.0, 'Northridge'),\n",
       " (0.0, 'Oklahoma'),\n",
       " (0.0, 'Omaha'),\n",
       " (0.0, 'Owasso'),\n",
       " (0.0, 'Phoenix'),\n",
       " (0.0, 'Portland'),\n",
       " (0.0, 'Raleigh'),\n",
       " (0.0, 'Redwood City'),\n",
       " (0.0, 'Remote'),\n",
       " (0.0, 'San Diego'),\n",
       " (0.0, 'Seattle'),\n",
       " (0.0, 'Spartanburg'),\n",
       " (0.0, 'St. Louis'),\n",
       " (0.0, 'Stamford'),\n",
       " (0.0, 'Thousand Oaks'),\n",
       " (0.0, 'Winter Park')]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(model.feature_importances_,features), key=lambda pair: pair[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "12d5edeb-a272-43a0-9977-d951f12fedfb"
   },
   "source": [
    "#### Take ~100 scraped entries with salaries. Convert them to use with your model and predict the salary - which entries have the highest predicted salaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Albany</th>\n",
       "      <th>Albuquerque</th>\n",
       "      <th>Alexandria</th>\n",
       "      <th>Anchorage</th>\n",
       "      <th>Arlington</th>\n",
       "      <th>Atlanta</th>\n",
       "      <th>Austin</th>\n",
       "      <th>Baltimore</th>\n",
       "      <th>Bellevue</th>\n",
       "      <th>Berkeley Heights</th>\n",
       "      <th>...</th>\n",
       "      <th>high_salary</th>\n",
       "      <th>salary_clean</th>\n",
       "      <th>analyst</th>\n",
       "      <th>statistician</th>\n",
       "      <th>engineer</th>\n",
       "      <th>machine_learning</th>\n",
       "      <th>research</th>\n",
       "      <th>mid_level</th>\n",
       "      <th>entry_level</th>\n",
       "      <th>senior_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>55000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>160000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>120000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Albany  Albuquerque  Alexandria  Anchorage  Arlington  Atlanta  Austin  \\\n",
       "19.0       0            0           0          0          0        0       0   \n",
       "36.0       0            0           0          0          0        0       0   \n",
       "54.0       0            0           0          0          0        0       0   \n",
       "56.0       0            0           0          0          0        0       0   \n",
       "94.0       0            0           0          0          0        0       0   \n",
       "\n",
       "      Baltimore  Bellevue  Berkeley Heights      ...       high_salary  \\\n",
       "19.0          0         0                 0      ...             False   \n",
       "36.0          0         0                 0      ...              True   \n",
       "54.0          0         0                 0      ...              True   \n",
       "56.0          0         0                 0      ...              True   \n",
       "94.0          0         0                 0      ...             False   \n",
       "\n",
       "      salary_clean  analyst  statistician  engineer  machine_learning  \\\n",
       "19.0         55000        0             0         0                 0   \n",
       "36.0        160000        0             0         0                 0   \n",
       "54.0        120000        0             0         0                 0   \n",
       "56.0        200000        0             0         0                 0   \n",
       "94.0         50000        0             0         0                 0   \n",
       "\n",
       "      research  mid_level  entry_level  senior_level  \n",
       "19.0         0          0            0             0  \n",
       "36.0         0          0            0             0  \n",
       "54.0         0          2            0             0  \n",
       "56.0         0          0            0             0  \n",
       "94.0         1          0            0             0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Bonus: Use Count Vectorizer from scikit-learn to create features from the text summaries. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "4239e458-28bd-4675-8db3-c1d9c02b9854"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "fec80936-37bc-4922-89bd-b5d615566c9c"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
